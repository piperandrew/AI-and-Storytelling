{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46273a7e-4863-42ba-8b34-c642a0ac2350",
   "metadata": {},
   "source": [
    "# Welcome!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18c7032b-19da-420e-be87-76c671e12d3b",
   "metadata": {},
   "source": [
    "Knight et al. (2024) use sentiment arcs to measure \"narrative reversals.\" Your task is to take a creative approach towards modeling \"narrative reversals.\" You will be a given a table of Liu's _Paper Menagerie_ divided into sentences. \n",
    "\n",
    "Part 1. try to replicate Knight et al's approach using the concept of sentiment valence. For each sentence ask the LLM to provide a sentiment score. \"Analyze the sentiment of the following sentence. Give your answer in the following form. 5 = very positive, 4 = somewhat positive, 3 = neutral, 2 = somewhat negative, 1 = very negative. Only answer with a number. Here is the sentence: {text}\". Feel free to play with the prompt to see what provides the most sensible answer.\n",
    "\n",
    "Part 2. To analyze your data you have two options. The first is output your .csv and upload it to Claude and ask it to visualize the data for you. Or give Claude the data and ask it to write a Python notebook script to visualize the data. How does the LLM-generated output compare to your own arcs?\n",
    "\n",
    "Part 3. Experiment with grouping sentences. Ask Claude to write a cell for your Jupyter notebook that aggregates sentences by a fixed number (N sentences, where you control N). Then run your prompt multiple times using different numbers of aggregated sentences and see how the results compare.\n",
    "\n",
    "Part 4. Experiment with other prompting frameworks for \"narrative reversal.\" Sky's the limit!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56838bb1-654e-461a-93f3-1460bd95ccdb",
   "metadata": {},
   "source": [
    "# 1. Install Prerequisite Libraries\n",
    "The below code will depend on three Python \"libraries\" (software collections). Run the below cell once to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269be1d0-9531-43eb-8a48-4b4b29a01951",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044b77d-8e22-4371-86b6-d98770674b28",
   "metadata": {},
   "source": [
    "# 2. Establish Your Working Directory\n",
    "\n",
    "For our projects this semester we will upload a .csv file that has a \"text\" column. This will be our input to the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bceb944-6e7f-4e57-8f6d-88f82bf61396",
   "metadata": {},
   "source": [
    "First establish your working directory. Create a folder called \"Jupyter\" and put it in your Documents folder. Then run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6dc77-3c12-4e5f-a3d7-40a2d19b3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we need\n",
    "from pathlib import Path  # This helps us work with file paths\n",
    "import os                # This lets us change directories\n",
    "\n",
    "def use_jupyter_folder():\n",
    "    # Get the path to the Jupyter folder\n",
    "    jupyter_folder = Path.home() / 'Documents' / 'Jupyter'\n",
    "    \n",
    "    # Try to change to that directory\n",
    "    if jupyter_folder.exists():\n",
    "        os.chdir(jupyter_folder)\n",
    "        print(f\"‚úÖ Now using your Jupyter folder!\")\n",
    "        print(f\"Current working directory: {Path.cwd()}\")\n",
    "    else:\n",
    "        print(\"‚ùå Couldn't find the Jupyter folder in Documents.\")\n",
    "        print(\"Please make sure you've created it first.\")\n",
    "\n",
    "# Run this to switch to the Jupyter folder\n",
    "use_jupyter_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325f954-867f-434d-8b27-c705d436d2ff",
   "metadata": {},
   "source": [
    "# 3. Upload Your Data\n",
    "\n",
    "Next upload a .csv file. Paste the filename where indicated. This cell will output the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c914d-5256-4820-a36e-272d7742766d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "FILENAME = \"NarraDetect_Scalar.csv\"  # Your CSV filename here\n",
    "\n",
    "## Define Function\n",
    "import pandas as pd\n",
    "\n",
    "def load_csv(filename):\n",
    "   \"\"\"Load CSV file and display info\"\"\"\n",
    "   try:\n",
    "       df = pd.read_csv(filename)\n",
    "       print(f\"‚úÖ Successfully loaded {filename}\")\n",
    "       print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "       print(\"\\nColumns in this dataset:\")\n",
    "       for col in df.columns:\n",
    "           print(f\"- {col}\")\n",
    "       return df\n",
    "   except FileNotFoundError:\n",
    "       print(f\"‚ùå Could not find {filename} in {Path.cwd()}\")\n",
    "   except Exception as e:\n",
    "       print(f\"‚ùå Error loading file: {str(e)}\")\n",
    "\n",
    "## Run function\n",
    "df = load_csv(FILENAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6447f07-9dc7-44d1-8dbf-dbc385ab7ac2",
   "metadata": {},
   "source": [
    "# 4. Inspect Your Data\n",
    "\n",
    "This cell will give you brief summary statistics on the input text column. This is the column you will use as part of your prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651fd89-83d1-4e8c-a0c0-30d5b07d264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "TEXT_COLUMN = 'TEXT'    # Column containing text data\n",
    "NUM_EXAMPLES = 2        # Number of example texts to display\n",
    "\n",
    "########## FUNCTION DEFINITION ###########\n",
    "def text_stats(df, text_column=TEXT_COLUMN, num_examples=NUM_EXAMPLES):\n",
    "   \"\"\"Display text statistics and examples\"\"\"\n",
    "   # Calculate word counts\n",
    "   word_counts = df[text_column].str.split().str.len()\n",
    "   total_words = word_counts.sum()\n",
    "   \n",
    "   print(f\"üìä Dataset Overview:\")\n",
    "   print(f\"Total number of texts: {len(df)}\")\n",
    "   \n",
    "   print(f\"\\nüìù Text Length Statistics:\")\n",
    "   print(f\"Shortest text: {word_counts.min()} words\")\n",
    "   print(f\"Longest text: {word_counts.max()} words\")\n",
    "   print(f\"Average length: {word_counts.mean():.1f} words\")\n",
    "   print(f\"Median length: {word_counts.median():.1f} words\")\n",
    "   print(f\"Total words in dataset: {total_words:,} words\")\n",
    "   \n",
    "   print(f\"\\nüìö Here are {num_examples} example texts from your data:\")\n",
    "   for i in range(num_examples):\n",
    "       idx = df.index[i]\n",
    "       text = df.loc[idx, text_column]\n",
    "       length = len(text.split())\n",
    "       print(f\"Example {i+1}:\")\n",
    "       print(f\"Length: {length} words\")\n",
    "       print(f\"Text: {text}\")\n",
    "\n",
    "# Calculate statistics and show examples\n",
    "text_stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a8195-5cd0-4327-b973-798635bc1c85",
   "metadata": {},
   "source": [
    "# 5. Define your Ollama model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708e543-17ef-4cf0-bd62-0d36ef15d728",
   "metadata": {},
   "source": [
    "You will run this cell only once for the semester. Once the model is loaded you don't need to run it again.\n",
    "But you do need to run it every time you want to test a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543dd310-844d-42b0-a033-0c37043e85d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = \"llama3:8b\"  # Change this to your model name, e.g. \"mistral\", \"codellama\", etc.\n",
    "#model = \"deepseek-r1:7b\"\n",
    "#!ollama pull {model}\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243f8f4-46db-4990-83df-33bf58dd645a",
   "metadata": {},
   "source": [
    "# 6. Prompt Testing\n",
    "\n",
    "In this cell you define your various parameters. These include your model, the column that has text passages, your prompt, and whether you want to use a structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2836ab-e713-4333-b94e-8a0ad1379ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUT YOUR PARAMETERS HERE #####\n",
    "MODEL_NAME = model \n",
    "COLUMN_NAME = \"TEXT\"   # Change dataframe column name here\n",
    "PROMPT_TEMPLATE = \"Is this passage from a story? Answer 1 for yes or 0 for no {text}\" #Change your prompt here\n",
    "STRUCTURED = False\n",
    "LABELS = [\"1\", \"0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4d8b4-655b-417d-9604-41ae9c74fd09",
   "metadata": {},
   "source": [
    "## 7. Test a random passage\n",
    "\n",
    "The cell chooses a random passage from the .csv and outputs the answer. You can run multiple times to keep testing answers on random passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f826ab-394d-47e1-8467-1457a56d53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import random\n",
    "import requests\n",
    "import ast\n",
    "\n",
    "def query_ollama(text):\n",
    "    \"\"\"Query local ollama model with text\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
    "    \n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"label\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\" : LABELS\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\n",
    "                    \"label\",\n",
    "                ]\n",
    "        } if STRUCTURED else ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if model exists\n",
    "        model_url = \"http://localhost:11434/api/tags\"\n",
    "        models = requests.get(model_url).json()\n",
    "        available_models = [model['name'] for model in models['models']]\n",
    "        \n",
    "        if MODEL_NAME not in available_models:\n",
    "            print(f\"‚ùå Model '{MODEL_NAME}' not found.\")\n",
    "            print(f\"Available models: {', '.join(available_models)}\")\n",
    "            print(f\"\\nTo install {MODEL_NAME}, run this in terminal:\")\n",
    "            print(f\"ollama pull {MODEL_NAME}\")\n",
    "            return None\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 404:\n",
    "            print(\"‚ùå Ollama service not running.\")\n",
    "            print(\"Start ollama by running 'ollama serve' in terminal\")\n",
    "            return None\n",
    "\n",
    "        result = response.json()\n",
    "        if STRUCTURED:\n",
    "            return ast.literal_eval(result['response'])['label']\n",
    "        return result['response']\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Cannot connect to Ollama\")\n",
    "        print(\"1. Check if Ollama is installed\") \n",
    "        print(\"2. Start Ollama by running 'ollama serve' in terminal\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_random_text(df):\n",
    "  \"\"\"Analyze a random text from dataset\"\"\"\n",
    "  random_idx = random.randint(0, len(df)-1)\n",
    "  text = df.iloc[random_idx][COLUMN_NAME]\n",
    "  print(\"\\nüìñ SAMPLE PASSAGE:\")\n",
    "  print(text)\n",
    "  print(\"\\nü§ñ MODEL RESPONSE:\")\n",
    "  return query_ollama(text)\n",
    "\n",
    "# Run\n",
    "result = analyze_random_text(df)\n",
    "if result:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab509a-1091-4a9d-8c54-313a63d2f137",
   "metadata": {},
   "source": [
    "# 8. Run your prompt on all of your data\n",
    "\n",
    "In this cell you will run your prompt on the full data. The outputs will be stored as a new column named after the model you are using. In the next cell you can view those results. The cell will output \"Completed\" when complete.\n",
    "\n",
    "** Note this takes parameters from Cell 6. Prompt Testing. If you want to change them go up and rerun that cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5653fab-414c-4b53-81b6-858001c27237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_ollama(text):\n",
    "    \"\"\"Query local ollama model with text\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
    "    \n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"label\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\" : LABELS\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\n",
    "                    \"label\",\n",
    "            ]\n",
    "        } if STRUCTURED else ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if model exists\n",
    "        model_url = \"http://localhost:11434/api/tags\"\n",
    "        models = requests.get(model_url).json()\n",
    "        available_models = [model['name'] for model in models['models']]\n",
    "        \n",
    "        if MODEL_NAME not in available_models:\n",
    "            print(f\"‚ùå Model '{MODEL_NAME}' not found.\")\n",
    "            return None\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 404:\n",
    "            print(\"‚ùå Ollama service not running.\")\n",
    "            return None\n",
    "\n",
    "        result = response.json()\n",
    "        if STRUCTURED:\n",
    "            return ast.literal_eval(result['response'])['label']\n",
    "        return result['response']\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Cannot connect to Ollama\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_all_texts(df):\n",
    "    \"\"\"Analyze all texts in the dataframe\"\"\"\n",
    "    # Create new column for responses using model name\n",
    "    df[MODEL_NAME] = df[COLUMN_NAME].apply(query_ollama)\n",
    "    return df\n",
    "\n",
    "# Run analysis on all rows\n",
    "sample_df = analyze_all_texts(df)\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d2933-8c13-4969-a986-6c8e1173a372",
   "metadata": {},
   "source": [
    "# 9. Inspect your outputs\n",
    "\n",
    "You can quickly scan your results by printing out the first N examples. Change the final integer to print more or less. Shows the passage + prompt output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11712ca-2305-4261-b62f-5e3d86c46331",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_df[[COLUMN_NAME, MODEL_NAME]].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb48fd7-e9bf-456b-8007-1e27d7a0ace4",
   "metadata": {},
   "source": [
    "Print a single passage by row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896da18f-a810-46fd-b718-de55af944351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a specific row (change row_number to view different rows)\n",
    "row_number = 2  # Change this number to view different rows\n",
    "print(f\"\\nDetailed view of row {row_number}:\")\n",
    "print(f\"\\nTEXT:\\n{sample_df[COLUMN_NAME].iloc[row_number]}\")\n",
    "print(f\"\\n{MODEL_NAME} response:\\n{sample_df[MODEL_NAME].iloc[row_number]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ce40f-9ec4-4053-9c2d-f79ebee60a6b",
   "metadata": {},
   "source": [
    "## 10. Clean your outputs to make sure they are numeric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5583fc-9925-481b-87bc-3c56bd136372",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_responses(df, model_column=MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Clean responses by extracting numbers\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the responses to clean\n",
    "    model_column : str\n",
    "        Name of the column containing responses to clean\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with new cleaned column added\n",
    "    \"\"\"\n",
    "    def standardize_response(response):\n",
    "        import re\n",
    "        numbers = re.findall(r'\\d+', str(response))\n",
    "        return numbers[0] if numbers else 'unknown'\n",
    "            \n",
    "    # Create new cleaned column\n",
    "    cleaned_column = f\"{model_column}_cleaned\"\n",
    "    df[cleaned_column] = df[model_column].apply(standardize_response)\n",
    "    \n",
    "    # Show the counts of each category \n",
    "    cleaned_counts = df[cleaned_column].value_counts()\n",
    "    print(f\"\\nCleaned response categories:\")\n",
    "    for category, count in cleaned_counts.items():\n",
    "        print(f\"{category}: {count}\")\n",
    "    print(f\"\\nTotal samples: {len(df)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean the responses\n",
    "sample_df = clean_responses(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a37ac1-a48b-487b-a64e-435162e87bce",
   "metadata": {},
   "source": [
    "## 11. Make a histogram of the distribution of labels.\n",
    "\n",
    "Ask Claude what a histogram is and why it is useful for understanding your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32b3125-93dc-4340-81e5-2e02e5ee7946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure the column is numeric (convert to int or float if needed)\n",
    "data = sample_df[f\"{MODEL_NAME}_cleaned\"][sample_df[f\"{MODEL_NAME}_cleaned\"] != 'unknown']\n",
    "data = data.astype(float)  # Convert to numeric if not already\n",
    "\n",
    "# Create the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(\n",
    "    data, \n",
    "    bins=[0.5, 1.5, 2.5, 3.5, 4.5, 5.5], \n",
    "    edgecolor='black',  # Adds borders to bars for clarity\n",
    "    align='mid'\n",
    ")\n",
    "\n",
    "# Set tick marks and axis labels\n",
    "plt.xticks([1, 2, 3, 4, 5])  # Ensure proper ordering of tick marks\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.title('Distribution of Numeric Responses', fontsize=16, pad=15)\n",
    "plt.xlabel('Sentiment', fontsize=14, labelpad=10)\n",
    "plt.ylabel('Frequency', fontsize=14, labelpad=10)\n",
    "\n",
    "# Add gridlines for readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()  # Ensures everything fits neatly\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593147d-931d-458b-9e86-29b1927f2150",
   "metadata": {},
   "source": [
    "## 12. Output your .csv to your Jupyter directory\n",
    "\n",
    "You can do this to inpsect your data more, review outputs or upload your data to Claude / GPT for visualization or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830cb24-e58c-4b83-a437-f3cebe948c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "output_file = \"sample_df_output.csv\"  # Specify the output file name\n",
    "sample_df.to_csv(output_file, index=False)  # Set index=False to exclude the DataFrame index\n",
    "\n",
    "print(f\"DataFrame saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef326bfb-af85-4760-b4c1-5126232d5e14",
   "metadata": {},
   "source": [
    "## 12. Visualize your plot arc\n",
    "\n",
    "Ask Claude or GPT to write a script for a cell that visualizes your cleaned output column. Tell it the name of the column (\"llama3:8b_cleaned\"), the nature of the data and the best way to visualize the data as a plot arc. Play with your queries to see how it changes results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ce1cb0-7189-4cc0-8772-20a95e8a2e52",
   "metadata": {},
   "source": [
    "## 13. Group sentences by N sentences\n",
    "\n",
    "Ask Claude or GPT to write a script that aggregates sentences by a variable that you input. So you input the value of N and it creates a new .csv with the aggregated sentences per row. Have it output the .csv to your directory. Then restart the process above using this new .csv and see what happens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34e4025-71e0-442f-86b8-b7e1dbfe9971",
   "metadata": {},
   "source": [
    "## 14. Restart using a different prompting framework.\n",
    "\n",
    "How else can you model \"narrative reversals\" by prompting your LLM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871e0903-91ae-4c9e-9b83-dbb0f27296cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
