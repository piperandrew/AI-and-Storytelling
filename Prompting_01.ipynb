{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46273a7e-4863-42ba-8b34-c642a0ac2350",
   "metadata": {},
   "source": [
    "# Welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c7032b-19da-420e-be87-76c671e12d3b",
   "metadata": {},
   "source": [
    "This notebook will allow you to customize prompts with different language models on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56838bb1-654e-461a-93f3-1460bd95ccdb",
   "metadata": {},
   "source": [
    "# Install Prerequisite Libraries\n",
    "The below code will depend on three Python \"libraries\" (software collections). Run the below cell once to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269be1d0-9531-43eb-8a48-4b4b29a01951",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044b77d-8e22-4371-86b6-d98770674b28",
   "metadata": {},
   "source": [
    "# Establish Your Working Directory\n",
    "\n",
    "For our projects this semester we will upload a .csv file that has a \"text\" column. This will be our input to the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bceb944-6e7f-4e57-8f6d-88f82bf61396",
   "metadata": {},
   "source": [
    "First establish your working directory. Create a folder called \"Jupyter\" and put it in your Documents folder. Then run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b6dc77-3c12-4e5f-a3d7-40a2d19b3a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Now using your Jupyter folder!\n",
      "Current working directory: /Users/akpiper/Documents/Jupyter\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries we need\n",
    "from pathlib import Path  # This helps us work with file paths\n",
    "import os                # This lets us change directories\n",
    "\n",
    "def use_jupyter_folder():\n",
    "    # Get the path to the Jupyter folder\n",
    "    jupyter_folder = Path.home() / 'Documents' / 'Jupyter'\n",
    "    \n",
    "    # Try to change to that directory\n",
    "    if jupyter_folder.exists():\n",
    "        os.chdir(jupyter_folder)\n",
    "        print(f\"âœ… Now using your Jupyter folder!\")\n",
    "        print(f\"Current working directory: {Path.cwd()}\")\n",
    "    else:\n",
    "        print(\"âŒ Couldn't find the Jupyter folder in Documents.\")\n",
    "        print(\"Please make sure you've created it first.\")\n",
    "\n",
    "# Run this to switch to the Jupyter folder\n",
    "use_jupyter_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325f954-867f-434d-8b27-c705d436d2ff",
   "metadata": {},
   "source": [
    "# Upload Your Data\n",
    "\n",
    "Next upload a .csv file of your choosing. Paste the filename where indicated at the bottom. This cell will output the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c2c914d-5256-4820-a36e-272d7742766d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully loaded NarraDetect_Scalar.csv\n",
      "Shape: 394 rows, 35 columns\n",
      "\n",
      "Columns in this dataset:\n",
      "- NARRATIVITY\n",
      "- GENRE\n",
      "- FILENAME\n",
      "- TEXT\n",
      "- AY_agency\n",
      "- AY_event\n",
      "- AY_world\n",
      "- AY_overall\n",
      "- ML_agency\n",
      "- ML_event\n",
      "- ML_world\n",
      "- ML_overall\n",
      "- LM_agency\n",
      "- LM_event\n",
      "- LM_world\n",
      "- LM_overall\n",
      "- avg_agency\n",
      "- avg_event_seq\n",
      "- avg_worldmaking\n",
      "- avg_overall\n",
      "- AvgDeviation\n",
      "- Reader.Predicted.Label\n",
      "- gpt.binary\n",
      "- gpt.ordinal\n",
      "- gpt.ordinal.mean\n",
      "- svm.probability\n",
      "- bert.probability\n",
      "- longform.probability\n",
      "- Llama.3.8B.ordinal\n",
      "- Llama.3.8B.ordinal.mean\n",
      "- Llama.3.8B.binary\n",
      "- Llama.3.8B.fine.tuned.binary\n",
      "- svm.binary\n",
      "- bert.binary\n",
      "- longform.binary\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_csv(filename):\n",
    "   \"\"\"Load CSV file and display info\"\"\"\n",
    "   try:\n",
    "       df = pd.read_csv(filename)\n",
    "       print(f\"âœ… Successfully loaded {filename}\")\n",
    "       print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "       print(\"\\nColumns in this dataset:\")\n",
    "       for col in df.columns:\n",
    "           print(f\"- {col}\")\n",
    "       return df\n",
    "   except FileNotFoundError:\n",
    "       print(f\"âŒ Could not find {filename} in {Path.cwd()}\")\n",
    "   except Exception as e:\n",
    "       print(f\"âŒ Error loading file: {str(e)}\")\n",
    "\n",
    "########## PASTE YOUR .CSV FILENAME HERE ###########\n",
    "df = load_csv(\"NarraDetect_Scalar.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6447f07-9dc7-44d1-8dbf-dbc385ab7ac2",
   "metadata": {},
   "source": [
    "# Inspect Your Data\n",
    "\n",
    "This cell will give you brief summary statistics on the input text column. This is the column you will use as part of your prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7651fd89-83d1-4e8c-a0c0-30d5b07d264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Dataset Overview:\n",
      "Total number of texts: 394\n",
      "\n",
      "ðŸ“ Text Length Statistics:\n",
      "Shortest text: 16 words\n",
      "Longest text: 444 words\n",
      "Average length: 121.9 words\n",
      "Median length: 111.0 words\n",
      "Total words in dataset: 48,017 words\n",
      "\n",
      "ðŸ“š Here are 2 example texts from your data:\n",
      "Example 1:\n",
      "Length: 187 words\n",
      "Text: This was a history lesson under erasure that translated Stalinist mass violence into late-Soviet social discipline. A full explanation of Khrushchevâ€™s speech and its aftermath and of the social processes surrounding the history and memory of Stalinist collective violence requires the articulation of novel theoretical positions. Analyses of legacies of mass historical violence in Europe and elsewhere typically cast the social arena as a battleground between memory and oblivion, projecting psychic mechanisms associated with the aftermath of traumaâ€”most frequently those of repression and repetition compulsionâ€”as metaphors for collective processes in which a society as a whole is conceived as repressing memory of past events, memory that must be brought to the fore if recovery and amelioration are to take place. YetÂ memory,Â forgetting, andÂ repressionÂ are precisely the wrong terms for analysis of legacies of Stalinist mass violence in the USSRâ€”and in present-day Russia as well. In this case, nothing is forgotten, and repressionâ€”with wooden clubs or exile to the campsâ€”bears little resemblance to repression of traumatic suffering, in which the original perception of unimaginable events is thought to remain unassimilated by memory, an inaccessible lost experience.\n",
      "Example 2:\n",
      "Length: 284 words\n",
      "Text: The text articulates for the Duchess â€œa world of her own invention,â€ but only out of or in relation to the common words, images, and knowledge of Cavendishâ€™s seventeenth-century England, the world of â€œsensitive and rational self-moving matterâ€ that Cavendish envisioned the earth to be. Crucially, the Duchess does not choose to make a world of philosophy or of abstracted forms, symbols, or ideas like the models she rejects, but instead a world of anthropomorphized matter, a foundational quality that renders humans and nature of the same cloth. As with all science fiction, the fantastical worlds constructed in oneâ€™s imagination only become coherent when expressed in fragments of common knowledge: Huxleyâ€™s flying cars in Brave New World are cars nevertheless, just like the purplish complexions of the beast-men in the Blazing World are only comprehensible to the Empress in terms of the skin colors of her native world. That the semiotics of science fiction works this way is not new; but that Cavendish chooses anthropomorphism in The Blazing World (and in Observations) as a distinct method for acknowledging and describing the ways in which nature is already animated is highly relevant to our contemporary studies of both the Anthropocene and Cavendish. Though one of the most formidable challenges for natural philosophers in Cavendishâ€™s early modern England was to systematize experimentation such that experimental science could more reliably earn Spratâ€™s rhetorical claim to â€œReal Knowledge,â€ perhaps one of the most formidable challenges of the Anthropocene age is to rethink the kind of â€œReal Knowledgeâ€ that positions nature as an inanimate object in relation to the human subject, and thus to learn how to articulate our world as animated with us, not in opposition to us.\n"
     ]
    }
   ],
   "source": [
    "#### DEFINE YOUR TEXT COLUMN HERE and NUMBER OF EXAMPLES YOU WANT TO SEE ######\n",
    "def text_stats(df, text_column='TEXT', num_examples=2):\n",
    "   \"\"\"Display text statistics and examples\"\"\"\n",
    "   # Calculate word counts\n",
    "   word_counts = df[text_column].str.split().str.len()\n",
    "   total_words = word_counts.sum()\n",
    "   \n",
    "   print(f\"ðŸ“Š Dataset Overview:\")\n",
    "   print(f\"Total number of texts: {len(df)}\")\n",
    "   \n",
    "   print(f\"\\nðŸ“ Text Length Statistics:\")\n",
    "   print(f\"Shortest text: {word_counts.min()} words\")\n",
    "   print(f\"Longest text: {word_counts.max()} words\")\n",
    "   print(f\"Average length: {word_counts.mean():.1f} words\")\n",
    "   print(f\"Median length: {word_counts.median():.1f} words\")\n",
    "   print(f\"Total words in dataset: {total_words:,} words\")\n",
    "   \n",
    "   print(f\"\\nðŸ“š Here are {num_examples} example texts from your data:\")\n",
    "   for i in range(num_examples):\n",
    "       idx = df.index[i]\n",
    "       text = df.loc[idx, text_column]\n",
    "       length = len(text.split())\n",
    "       print(f\"Example {i+1}:\")\n",
    "       print(f\"Length: {length} words\")\n",
    "       print(f\"Text: {text}\")\n",
    "\n",
    "# Calculate statistics and show examples\n",
    "text_stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a8195-5cd0-4327-b973-798635bc1c85",
   "metadata": {},
   "source": [
    "# Ensure a Model is Loaded in Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708e543-17ef-4cf0-bd62-0d36ef15d728",
   "metadata": {},
   "source": [
    "You will run this cell only once for the semester. Once the model is loaded you don't need to run it again.\n",
    "But you do need to run it every time you want to test a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543dd310-844d-42b0-a033-0c37043e85d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3:8b\"  # Change this to your model name, e.g. \"mistral\", \"codellama\", etc.\n",
    "#!ollama pull {model} >/dev/null 2>&1\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4d8b4-655b-417d-9604-41ae9c74fd09",
   "metadata": {},
   "source": [
    "# Prompt Testing\n",
    "\n",
    "In this cell you can choose a model from Ollama and then customize a prompt. You also need to specify the text column from your .csv. The cell chooses a random passage from the .csv and outputs the answer. You can run multiple times to keep testing answers on random passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1f826ab-394d-47e1-8467-1457a56d53f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“– SAMPLE PASSAGE:\n",
      "After his mother rescues him, he joins his swarm of siblings flying above and is bumped and jostled. Skinny even by mosquito standards, and fearing he will end up doused again, Dinnn decides he would rather walk than fly.,This decision inevitably leads to Dinnnâ€™s being taunted by older mosquitos and largely forgotten by his family. The ridicule intensifies when Dinnn finds a tiny, discarded motorcycle jacket (with â€œWasp Brothersâ€ inscribed on the back), which he takes to wearing over his unused wings.,Dinnn is put into a situation that forces him to ditch his jacket and take flight, which allows him to tag along with the rest of his family as they hitch a ride in a minivan to the â€œwild,â€ where his mother came from, and where his half-brother still resides.,The transition from the first part of the narrative to the latter sections (after Dinnn learns to fly) is less smooth than it could be. The young bugâ€™s fear of flying is suddenly and easily resolved, and lends a note of finality to the tale. When the story then starts heading in a different direction (with new issues for Dinnn to confront), there is a feeling of disconnection from what has come before.\n",
      "\n",
      "ðŸ¤– MODEL RESPONSE:\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import random\n",
    "import requests\n",
    "import ast\n",
    "\n",
    "##### INPUT YOUR PARAMETERS HERE #####\n",
    "MODEL_NAME = \"llama3:8b\"  # Change model name here.\n",
    "COLUMN_NAME = \"TEXT\"   # Change dataframe column name here\n",
    "PROMPT_TEMPLATE = \"Is this passage from a story? Answer only yes or no. {text}\" #Change your prompt here\n",
    "STRUCTURED = True\n",
    "LABELS = [\"Yes\", \"No\"]\n",
    "\n",
    "def query_ollama(text):\n",
    "    \"\"\"Query local ollama model with text\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
    "    \n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"label\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\" : LABELS\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\n",
    "                    \"label\",\n",
    "                ]\n",
    "        } if STRUCTURED else ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if model exists\n",
    "        model_url = \"http://localhost:11434/api/tags\"\n",
    "        models = requests.get(model_url).json()\n",
    "        available_models = [model['name'] for model in models['models']]\n",
    "        \n",
    "        if MODEL_NAME not in available_models:\n",
    "            print(f\"âŒ Model '{MODEL_NAME}' not found.\")\n",
    "            print(f\"Available models: {', '.join(available_models)}\")\n",
    "            print(f\"\\nTo install {MODEL_NAME}, run this in terminal:\")\n",
    "            print(f\"ollama pull {MODEL_NAME}\")\n",
    "            return None\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 404:\n",
    "            print(\"âŒ Ollama service not running.\")\n",
    "            print(\"Start ollama by running 'ollama serve' in terminal\")\n",
    "            return None\n",
    "\n",
    "        result = response.json()\n",
    "        if STRUCTURED:\n",
    "            return ast.literal_eval(result['response'])['label']\n",
    "        return result['response']\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Cannot connect to Ollama\")\n",
    "        print(\"1. Check if Ollama is installed\") \n",
    "        print(\"2. Start Ollama by running 'ollama serve' in terminal\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_random_text(df):\n",
    "  \"\"\"Analyze a random text from dataset\"\"\"\n",
    "  random_idx = random.randint(0, len(df)-1)\n",
    "  text = df.iloc[random_idx][COLUMN_NAME]\n",
    "  print(\"\\nðŸ“– SAMPLE PASSAGE:\")\n",
    "  print(text)\n",
    "  print(\"\\nðŸ¤– MODEL RESPONSE:\")\n",
    "  return query_ollama(text)\n",
    "\n",
    "# Run\n",
    "result = analyze_random_text(df)\n",
    "if result:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a9bf5-c088-4362-aeae-aaa88cdb2605",
   "metadata": {},
   "source": [
    "# Sample your data\n",
    "\n",
    "In this cell you will downsample your .csv file to run a mini test in class. For your final report you will run the model(s) against all rows (or a minimum sample of 100). This function allows you to determine the number of rows you sample and stores the new table. Change the variable \"n\" for number of rows to sample. \n",
    "\n",
    "** Note: every time you run this cell you will get a new random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960d4004-05a5-4f45-933e-586922cf29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_texts(df, n=20): # change n here to adjust the size of your sample\n",
    "    \"\"\"\n",
    "    Sample n random rows from the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Your dataset\n",
    "    n (int): Number of samples to take\n",
    "    \"\"\"\n",
    "    global sample_df\n",
    "    sample_df = df.sample(n=n)\n",
    "    \n",
    "# Create sample with 3 rows\n",
    "sample_texts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab509a-1091-4a9d-8c54-313a63d2f137",
   "metadata": {},
   "source": [
    "# Run your prompt on your sample data\n",
    "\n",
    "In this cell you will run your prompt on the sampled data from above. The outputs will be stored as a new column named after the model you are using. In the next cell you can view those results. The cell will output \"Completed\" when complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5653fab-414c-4b53-81b6-858001c27237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "##### INPUT YOUR PARAMETERS HERE #####\n",
    "MODEL_NAME = \"llama3:8b\"  # Change model name here.\n",
    "COLUMN_NAME = \"TEXT\"   # Change dataframe column name here\n",
    "PROMPT_TEMPLATE = \"Is this passage from a story? Answer only yes or no. {text}\"\n",
    "STRUCTURED = True\n",
    "LABELS = [\"Yes\", \"No\"]\n",
    "\n",
    "def query_ollama(text):\n",
    "    \"\"\"Query local ollama model with text\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
    "    \n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"label\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\" : LABELS\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\n",
    "                    \"label\",\n",
    "            ]\n",
    "        } if STRUCTURED else ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if model exists\n",
    "        model_url = \"http://localhost:11434/api/tags\"\n",
    "        models = requests.get(model_url).json()\n",
    "        available_models = [model['name'] for model in models['models']]\n",
    "        \n",
    "        if MODEL_NAME not in available_models:\n",
    "            print(f\"âŒ Model '{MODEL_NAME}' not found.\")\n",
    "            return None\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 404:\n",
    "            print(\"âŒ Ollama service not running.\")\n",
    "            return None\n",
    "\n",
    "        result = response.json()\n",
    "        if STRUCTURED:\n",
    "            return ast.literal_eval(result['response'])['label']\n",
    "        return result['response']\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Cannot connect to Ollama\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_all_texts(df):\n",
    "    \"\"\"Analyze all texts in the dataframe\"\"\"\n",
    "    # Create new column for responses using model name\n",
    "    df[MODEL_NAME] = df[COLUMN_NAME].apply(query_ollama)\n",
    "    return df\n",
    "\n",
    "# Run analysis on all rows\n",
    "sample_df = analyze_all_texts(sample_df)\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d2933-8c13-4969-a986-6c8e1173a372",
   "metadata": {},
   "source": [
    "# Inspect your outputs\n",
    "\n",
    "You can quickly scan your results by printing out the first N examples. Change the final integer to print more or less. Shows the passage + prompt output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a11712ca-2305-4261-b62f-5e3d86c46331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  TEXT llama3:8b\n",
      "79   It just means white man,â€ says Kyle when Jared...       Yes\n",
      "267  That's a very different thing. But when it com...       Yes\n",
      "29   Purpose - The aim of this paper is to examine ...       Yes\n",
      "107  Resolving to dance at an upcoming powwow, John...       Yes\n",
      "2    Uli painting does not furnish indigenous subje...       Yes\n"
     ]
    }
   ],
   "source": [
    "print(sample_df[[COLUMN_NAME, MODEL_NAME]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb48fd7-e9bf-456b-8007-1e27d7a0ace4",
   "metadata": {},
   "source": [
    "Print a single passage by row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "896da18f-a810-46fd-b718-de55af944351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed view of row 3:\n",
      "\n",
      "TEXT:\n",
      "Resolving to dance at an upcoming powwow, John finds a way to stand up for himself so he can keep dancing and playing soccer.,John is an appealing character, a passionate teenager who works hard at both his prevailing interests. While he becomes a great dancer a little too quickly, his initial struggles are entertaining and realistic. Scenes between him and his parents and energetic younger sister, Jen, are especially well drawn, and the bookâ€™s supportive family dynamic contrasts well with the racism and conflict mentioned elsewhere. Johnâ€™s relationships with his friends, however, are lacking, and more space could have been devoted to his friendship with soccer teammate Aiden, who is understanding of Johnâ€™s choices but is an otherwise flat character.,While the book is not overly focused on racism, passing mention of some of John and Jenâ€™s negative experiences, and acknowledgement of topics like cultural appropriation, help ground the story without feeling heavy handed. ,He Who Dreams, offers readers a fast-paced story with realistic indigenous content, connecting the book to contemporary discussions about indigenous issues in Canada.\n",
      "\n",
      "llama3:8b response:\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "# Display a specific row (change row_number to view different rows)\n",
    "row_number = 3  # Change this number to view different rows\n",
    "print(f\"\\nDetailed view of row {row_number}:\")\n",
    "print(f\"\\nTEXT:\\n{sample_df[COLUMN_NAME].iloc[row_number]}\")\n",
    "print(f\"\\n{MODEL_NAME} response:\\n{sample_df[MODEL_NAME].iloc[row_number]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f48fdd-7cf9-4a60-a047-838a7002f197",
   "metadata": {},
   "source": [
    "# Compare your outputs to another reference column\n",
    "\n",
    "In the following cells you will compare the accuracy of your outputs to already annotated data. First you need to identify the \"reference\" column. These are the annotations. Second, you need to align your outputs with those of the reference column. Typically these will consist of a few number of codes. So the first step is finding out these codes so you can align them with your outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3828e854-8d8a-439e-9889-0fe4594f1506",
   "metadata": {},
   "source": [
    "## What are the annotation categories of my data\n",
    "\n",
    "Output a table of the categories and their counts in your data. Change the reference column name accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f7a35d66-9ccd-4a34-bc71-7fc6a6744ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories and their counts:\n",
      "0: 13\n",
      "1: 7\n",
      "\n",
      "Total samples: 20\n"
     ]
    }
   ],
   "source": [
    "###### Get value counts of the reference column. \n",
    "###### CHANGE reference column name based on yoru data. \n",
    "######It comes after df[\"reference_name\"].value_counts()\n",
    "reference_counts = sample_df[\"Reader.Predicted.Label\"].value_counts()\n",
    "\n",
    "print(\"Categories and their counts:\")\n",
    "for category, count in reference_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "print(f\"\\nTotal samples: {len(sample_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2c464-0fc3-4500-8d4b-81afd8c5cb11",
   "metadata": {},
   "source": [
    "Do the same thing for your model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8593b441-3c7f-49b8-a6a2-cf154bdbd586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response categories from llama3:8b:\n",
      "Yes: 17\n",
      "No: 3\n",
      "\n",
      "Total samples: 20\n"
     ]
    }
   ],
   "source": [
    "# Get value counts of the model responses\n",
    "model_counts = sample_df[MODEL_NAME].value_counts()\n",
    "\n",
    "print(f\"Response categories from {MODEL_NAME}:\")\n",
    "for category, count in model_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "print(f\"\\nTotal samples: {len(sample_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a389f1-4575-4799-9131-058060a90ed0",
   "metadata": {},
   "source": [
    "## Clean your model outputs\n",
    "\n",
    "In order to align your outputs with the reference column you first need to standardize your outputs. This cell outputs all of the types of response you got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76396a45-9df6-4005-8f4a-dfab038b24e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned response categories:\n",
      "Yes: 17\n",
      "No: 3\n",
      "\n",
      "Total samples: 20\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_responses(df, model_column=MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Clean and standardize model responses to 'Yes' or 'No' using case-insensitive pattern matching\n",
    "    \"\"\"\n",
    "    def standardize_response(response):\n",
    "        # Case insensitive search for 'yes' or 'no'\n",
    "        if re.search(r'yes', response, re.IGNORECASE):\n",
    "            return 'Yes'\n",
    "        elif re.search(r'no', response, re.IGNORECASE):\n",
    "            return 'No'\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected response format: '{response}'\")\n",
    "            return 'Unknown'\n",
    "            \n",
    "    # Create new cleaned column\n",
    "    cleaned_column = f\"{model_column}_cleaned\"\n",
    "    df[cleaned_column] = df[model_column].apply(standardize_response)\n",
    "    return df\n",
    "\n",
    "# Clean the responses\n",
    "sample_df = clean_responses(sample_df)\n",
    "\n",
    "# Show the new counts\n",
    "cleaned_counts = sample_df[f\"{MODEL_NAME}_cleaned\"].value_counts()\n",
    "print(f\"Cleaned response categories:\")\n",
    "for category, count in cleaned_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "print(f\"\\nTotal samples: {len(sample_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd86167-7185-4dea-9351-65d6833d2bd2",
   "metadata": {},
   "source": [
    "## Align outputs with reference column\n",
    "\n",
    "Next transform the reference column categories to match your model outputs. This is going to be a custom job every time. Paste the current code into Claude or GPT and ask for help changing the code based on your situation. Give the AI as much information as possible to help you and then paste the code back in here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1de8b19-f7ec-4658-86c5-b6ace82efa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference label categories:\n",
      "No: 13\n",
      "Yes: 7\n",
      "\n",
      "Total samples: 20\n"
     ]
    }
   ],
   "source": [
    "def transform_reference_labels(df):\n",
    "    \"\"\"\n",
    "    Transform reference labels from 1/0 to Yes/No\n",
    "    \"\"\"\n",
    "    def map_label(value):\n",
    "        if value == 1:\n",
    "            return 'Yes'\n",
    "        elif value == 0:\n",
    "            return 'No'\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected reference value: {value}\")\n",
    "            return 'Unknown'\n",
    "            \n",
    "    # Create new transformed column\n",
    "    reference_cleaned = \"Reader.Predicted.Label_cleaned\"\n",
    "    df[reference_cleaned] = df[\"Reader.Predicted.Label\"].apply(map_label)\n",
    "    return df\n",
    "\n",
    "# Transform reference labels\n",
    "sample_df = transform_reference_labels(sample_df)\n",
    "\n",
    "# Show the new counts\n",
    "reference_counts = sample_df[\"Reader.Predicted.Label_cleaned\"].value_counts()\n",
    "print(f\"Reference label categories:\")\n",
    "for category, count in reference_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "print(f\"\\nTotal samples: {len(sample_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c8ff0-bcc6-4ea6-a4af-d5ac7e08ac8b",
   "metadata": {},
   "source": [
    "# Calculate Precision, Recall, and F1 Score\n",
    "\n",
    "These are measures of agreement we will use this semester to see how well a model + prompt performs. Make sure to adjust the column names below to match your goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "50bfd4bb-15d1-4678-85bb-e3f570e40a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for llama3:8b:\n",
      "Precision: 0.412\n",
      "Recall: 1.000\n",
      "F1 Score: 0.583\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(df, model_column=MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1 score for model predictions\n",
    "    \"\"\"\n",
    "    ##### CHANGE COLUMN NAMES ######\n",
    "    y_true = df['Reader.Predicted.Label_cleaned'] ### This column is the original annotated data that has been transformed.\n",
    "    y_pred = df[f'{model_column}_cleaned'] ### This is your data that has been cleaned.\n",
    "    \n",
    "    # Convert to binary format for sklearn (Yes -> 1, No -> 0)\n",
    "    y_true_binary = (y_true == 'Yes').astype(int)\n",
    "    y_pred_binary = (y_pred == 'Yes').astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true_binary, y_pred_binary)\n",
    "    recall = recall_score(y_true_binary, y_pred_binary)\n",
    "    f1 = f1_score(y_true_binary, y_pred_binary)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Metrics for {model_column}:\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    \n",
    "# Calculate metrics\n",
    "calculate_metrics(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52229b-dd49-4bd1-a0e3-4aed1348f593",
   "metadata": {},
   "source": [
    "## Output a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9c0c914-478b-4881-aea3-b2a4548b8ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "Model: llama3:8b\n",
      "          Predicted Yes  Predicted No\n",
      "True Yes              7             0\n",
      "True No              10             3\n",
      "\n",
      "Reading the matrix:\n",
      "True Positives (Correct Yes): 7\n",
      "False Negatives (Missed Yes): 0\n",
      "False Positives (Wrong Yes): 10\n",
      "True Negatives (Correct No): 3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def display_confusion_matrix(df, model_column=MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Create and display a confusion matrix comparing model predictions to reference labels\n",
    "    \"\"\"\n",
    "    # Get the cleaned columns\n",
    "    y_true = df['Reader.Predicted.Label_cleaned']\n",
    "    y_pred = df[f'{model_column}_cleaned']\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=['Yes', 'No'])\n",
    "    \n",
    "    # Convert to pandas DataFrame for better display\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm, \n",
    "        index=['True Yes', 'True No'],\n",
    "        columns=['Predicted Yes', 'Predicted No']\n",
    "    )\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"Model: {model_column}\")\n",
    "    print(cm_df)\n",
    "    print(\"\\nReading the matrix:\")\n",
    "    print(f\"True Positives (Correct Yes): {cm[0,0]}\")\n",
    "    print(f\"False Negatives (Missed Yes): {cm[0,1]}\")\n",
    "    print(f\"False Positives (Wrong Yes): {cm[1,0]}\")\n",
    "    print(f\"True Negatives (Correct No): {cm[1,1]}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "display_confusion_matrix(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fccdd3d-079c-4bc3-bf2f-778ad9552a30",
   "metadata": {},
   "source": [
    "## Inspect errors\n",
    "\n",
    "Your errors can take the form of false positives (e.g. when the model thinks a passage is a story but isn't) or false negatives (e.g. when your model thinks the passage isn't a story but is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "699ed179-366a-49b5-a221-0fa5856a89af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FALSE POSITIVE EXAMPLE ===\n",
      "(Model incorrectly predicted Yes when the true label was No)\n",
      "\n",
      "Passage:\n",
      "What makes you call real life confusion is that it presents, as if they were dissolved in one another, a lot of differents which conception breaks life's flow by keeping apart. But _are_ not differents actually dissolved in one another? Hasn't every bit of experience its quality, its duration, its extension, its intensity, its urgency, its clearness, and many aspects besides, no one of which can exist in the isolation in which our verbalized logic keeps it? They exist only _durcheinander_. Reality always is, in M. Bergson's phrase, an endosmosis or conflux of the same with the different: they compenetrate and telescope.\n",
      "\n",
      "Model response:\n",
      "Yes\n",
      "\n",
      "=== FALSE NEGATIVE EXAMPLE ===\n",
      "(Model incorrectly predicted No when the true label was Yes)\n",
      "No false negatives found!\n"
     ]
    }
   ],
   "source": [
    "def show_error_examples(df, model_column=MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Display examples of false positives and false negatives with their corresponding text passages\n",
    "    \"\"\"\n",
    "    # Get relevant columns\n",
    "    ref_col = 'Reader.Predicted.Label_cleaned'\n",
    "    pred_col = f'{model_column}_cleaned'\n",
    "    \n",
    "    # Find false positives (model predicted Yes when true label was No)\n",
    "    false_positives = df[\n",
    "        (df[pred_col] == 'Yes') & \n",
    "        (df[ref_col] == 'No')\n",
    "    ]\n",
    "    \n",
    "    # Find false negatives (model predicted No when true label was Yes)\n",
    "    false_negatives = df[\n",
    "        (df[pred_col] == 'No') & \n",
    "        (df[ref_col] == 'Yes')\n",
    "    ]\n",
    "    \n",
    "    # Display one example of each if available\n",
    "    print(\"=== FALSE POSITIVE EXAMPLE ===\")\n",
    "    print(\"(Model incorrectly predicted Yes when the true label was No)\")\n",
    "    if len(false_positives) > 0:\n",
    "        fp_example = false_positives.sample(1).iloc[0]\n",
    "        print(f\"\\nPassage:\")\n",
    "        print(fp_example['TEXT'])\n",
    "        print(f\"\\nModel response:\")\n",
    "        print(fp_example[model_column])\n",
    "    else:\n",
    "        print(\"No false positives found!\")\n",
    "        \n",
    "    print(\"\\n=== FALSE NEGATIVE EXAMPLE ===\")\n",
    "    print(\"(Model incorrectly predicted No when the true label was Yes)\")\n",
    "    if len(false_negatives) > 0:\n",
    "        fn_example = false_negatives.sample(1).iloc[0]\n",
    "        print(f\"\\nPassage:\")\n",
    "        print(fn_example['TEXT'])\n",
    "        print(f\"\\nModel response:\")\n",
    "        print(fn_example[model_column])\n",
    "    else:\n",
    "        print(\"No false negatives found!\")\n",
    "\n",
    "# Show error examples\n",
    "show_error_examples(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca4e67-fe46-488b-8fb9-b8bcf14c2983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
