{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46273a7e-4863-42ba-8b34-c642a0ac2350",
   "metadata": {},
   "source": [
    "# Welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c7032b-19da-420e-be87-76c671e12d3b",
   "metadata": {},
   "source": [
    "This notebook will allow you to customize prompts with different language models on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56838bb1-654e-461a-93f3-1460bd95ccdb",
   "metadata": {},
   "source": [
    "# 1. Install Prerequisite Libraries\n",
    "The below code will depend on three Python \"libraries\" (software collections). Run the below cell once to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269be1d0-9531-43eb-8a48-4b4b29a01951",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install requests pandas scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044b77d-8e22-4371-86b6-d98770674b28",
   "metadata": {},
   "source": [
    "# 2. Establish Your Working Directory\n",
    "\n",
    "For our projects this semester we will upload a .csv file that has a \"text\" column. This will be our input to the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bceb944-6e7f-4e57-8f6d-88f82bf61396",
   "metadata": {},
   "source": [
    "First establish your working directory. Create a folder called \"Jupyter\" and put it in your Documents folder. Then run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6dc77-3c12-4e5f-a3d7-40a2d19b3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we need\n",
    "from pathlib import Path  # This helps us work with file paths\n",
    "import os                # This lets us change directories\n",
    "\n",
    "def use_jupyter_folder():\n",
    "    # Get the path to the Jupyter folder\n",
    "    jupyter_folder = Path.home() / 'Documents' / 'Jupyter'\n",
    "    \n",
    "    # Try to change to that directory\n",
    "    if jupyter_folder.exists():\n",
    "        os.chdir(jupyter_folder)\n",
    "        print(f\"âœ… Now using your Jupyter folder!\")\n",
    "        print(f\"Current working directory: {Path.cwd()}\")\n",
    "    else:\n",
    "        print(\"âŒ Couldn't find the Jupyter folder in Documents.\")\n",
    "        print(\"Please make sure you've created it first.\")\n",
    "\n",
    "# Run this to switch to the Jupyter folder\n",
    "use_jupyter_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325f954-867f-434d-8b27-c705d436d2ff",
   "metadata": {},
   "source": [
    "# 3. Upload Your Data\n",
    "\n",
    "Next upload a .csv file of your choosing. Paste the filename where indicated at the bottom. This cell will output the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c914d-5256-4820-a36e-272d7742766d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "FILENAME = \"NarraDetect_Scalar.csv\"  # Your CSV filename here\n",
    "\n",
    "## Define Function\n",
    "import pandas as pd\n",
    "\n",
    "def load_csv(filename):\n",
    "   \"\"\"Load CSV file and display info\"\"\"\n",
    "   try:\n",
    "       df = pd.read_csv(filename)\n",
    "       print(f\"âœ… Successfully loaded {filename}\")\n",
    "       print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "       print(\"\\nColumns in this dataset:\")\n",
    "       for col in df.columns:\n",
    "           print(f\"- {col}\")\n",
    "       return df\n",
    "   except FileNotFoundError:\n",
    "       print(f\"âŒ Could not find {filename} in {Path.cwd()}\")\n",
    "   except Exception as e:\n",
    "       print(f\"âŒ Error loading file: {str(e)}\")\n",
    "\n",
    "## Run function\n",
    "df = load_csv(FILENAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6447f07-9dc7-44d1-8dbf-dbc385ab7ac2",
   "metadata": {},
   "source": [
    "# 4. Inspect Your Data\n",
    "\n",
    "This cell will give you brief summary statistics on the input text column. This is the column you will use as part of your prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651fd89-83d1-4e8c-a0c0-30d5b07d264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "TEXT_COLUMN = 'TEXT'    # Column containing text data\n",
    "NUM_EXAMPLES = 2        # Number of example texts to display\n",
    "\n",
    "########## FUNCTION DEFINITION ###########\n",
    "def text_stats(df, text_column=TEXT_COLUMN, num_examples=NUM_EXAMPLES):\n",
    "   \"\"\"Display text statistics and examples\"\"\"\n",
    "   # Calculate word counts\n",
    "   word_counts = df[text_column].str.split().str.len()\n",
    "   total_words = word_counts.sum()\n",
    "   \n",
    "   print(f\"ðŸ“Š Dataset Overview:\")\n",
    "   print(f\"Total number of texts: {len(df)}\")\n",
    "   \n",
    "   print(f\"\\nðŸ“ Text Length Statistics:\")\n",
    "   print(f\"Shortest text: {word_counts.min()} words\")\n",
    "   print(f\"Longest text: {word_counts.max()} words\")\n",
    "   print(f\"Average length: {word_counts.mean():.1f} words\")\n",
    "   print(f\"Median length: {word_counts.median():.1f} words\")\n",
    "   print(f\"Total words in dataset: {total_words:,} words\")\n",
    "   \n",
    "   print(f\"\\nðŸ“š Here are {num_examples} example texts from your data:\")\n",
    "   for i in range(num_examples):\n",
    "       idx = df.index[i]\n",
    "       text = df.loc[idx, text_column]\n",
    "       length = len(text.split())\n",
    "       print(f\"Example {i+1}:\")\n",
    "       print(f\"Length: {length} words\")\n",
    "       print(f\"Text: {text}\")\n",
    "\n",
    "# Calculate statistics and show examples\n",
    "text_stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a8195-5cd0-4327-b973-798635bc1c85",
   "metadata": {},
   "source": [
    "# 5. Define your Ollama model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708e543-17ef-4cf0-bd62-0d36ef15d728",
   "metadata": {},
   "source": [
    "You will run this cell only once for the semester. Once the model is loaded you don't need to run it again.\n",
    "But you do need to run it every time you want to test a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543dd310-844d-42b0-a033-0c37043e85d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = \"llama3:8b\"  # Change this to your model name, e.g. \"mistral\", \"codellama\", etc.\n",
    "#model = \"deepseek-r1:7b\"\n",
    "#!ollama pull {model}\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243f8f4-46db-4990-83df-33bf58dd645a",
   "metadata": {},
   "source": [
    "# 6. Prompt Testing\n",
    "\n",
    "In this cell you define your various parameters. These include your model, the column that has text passages, your prompt, and whether you want to use a structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2836ab-e713-4333-b94e-8a0ad1379ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUT YOUR PARAMETERS HERE #####\n",
    "MODEL_NAME = model \n",
    "COLUMN_NAME = \"TEXT\"   # Change dataframe column name here\n",
    "PROMPT_TEMPLATE = \"Is this passage from a story? Answer 1 for yes or 0 for no {text}\" #Change your prompt here\n",
    "STRUCTURED = False\n",
    "LABELS = [\"1\", \"0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4d8b4-655b-417d-9604-41ae9c74fd09",
   "metadata": {},
   "source": [
    "## 7. Test a random passage\n",
    "\n",
    "The cell chooses a random passage from the .csv and outputs the answer. You can run multiple times to keep testing answers on random passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f826ab-394d-47e1-8467-1457a56d53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import random\n",
    "import requests\n",
    "import ast\n",
    "\n",
    "def query_ollama(text):\n",
    "    \"\"\"Query local ollama model with text\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
    "    \n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"label\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\" : LABELS\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\n",
    "                    \"label\",\n",
    "                ]\n",
    "        } if STRUCTURED else ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if model exists\n",
    "        model_url = \"http://localhost:11434/api/tags\"\n",
    "        models = requests.get(model_url).json()\n",
    "        available_models = [model['name'] for model in models['models']]\n",
    "        \n",
    "        if MODEL_NAME not in available_models:\n",
    "            print(f\"âŒ Model '{MODEL_NAME}' not found.\")\n",
    "            print(f\"Available models: {', '.join(available_models)}\")\n",
    "            print(f\"\\nTo install {MODEL_NAME}, run this in terminal:\")\n",
    "            print(f\"ollama pull {MODEL_NAME}\")\n",
    "            return None\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 404:\n",
    "            print(\"âŒ Ollama service not running.\")\n",
    "            print(\"Start ollama by running 'ollama serve' in terminal\")\n",
    "            return None\n",
    "\n",
    "        result = response.json()\n",
    "        if STRUCTURED:\n",
    "            return ast.literal_eval(result['response'])['label']\n",
    "        return result['response']\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Cannot connect to Ollama\")\n",
    "        print(\"1. Check if Ollama is installed\") \n",
    "        print(\"2. Start Ollama by running 'ollama serve' in terminal\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_random_text(df):\n",
    "  \"\"\"Analyze a random text from dataset\"\"\"\n",
    "  random_idx = random.randint(0, len(df)-1)\n",
    "  text = df.iloc[random_idx][COLUMN_NAME]\n",
    "  print(\"\\nðŸ“– SAMPLE PASSAGE:\")\n",
    "  print(text)\n",
    "  print(\"\\nðŸ¤– MODEL RESPONSE:\")\n",
    "  return query_ollama(text)\n",
    "\n",
    "# Run\n",
    "result = analyze_random_text(df)\n",
    "if result:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a9bf5-c088-4362-aeae-aaa88cdb2605",
   "metadata": {},
   "source": [
    "# 8. Sample your data\n",
    "\n",
    "In this cell you will downsample your .csv file to run a mini test in class. For your final report you will run the model(s) against all rows (or a minimum sample of 100 where there are more than 100). This function allows you to determine the number of rows you sample and stores the new table.\n",
    "\n",
    "** Note: every time you run this cell you will get a new random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960d4004-05a5-4f45-933e-586922cf29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "SAMPLE_SIZE = 20  # Number of random texts to sample\n",
    "\n",
    "########## FUNCTION DEFINITION ###########\n",
    "def sample_texts(df, n=SAMPLE_SIZE):\n",
    "    \"\"\"\n",
    "    Sample n random rows from the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Your dataset\n",
    "    n (int): Number of samples to take\n",
    "    \"\"\"\n",
    "    global sample_df\n",
    "    sample_df = df.sample(n=n)\n",
    "    \n",
    "# Create sample with 3 rows\n",
    "sample_texts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab509a-1091-4a9d-8c54-313a63d2f137",
   "metadata": {},
   "source": [
    "# 9. Run your prompt on your sample data\n",
    "\n",
    "In this cell you will run your prompt on the sampled data from above. The outputs will be stored as a new column named after the model you are using. In the next cell you can view those results. The cell will output \"Completed\" when complete.\n",
    "\n",
    "** Note this takes parameters from Cell 6. Prompt Testing. If you want to change them go up and rerun that cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5653fab-414c-4b53-81b6-858001c27237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_ollama(text):\n",
    "    \"\"\"Query local ollama model with text\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
    "    \n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"label\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\" : LABELS\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\n",
    "                    \"label\",\n",
    "            ]\n",
    "        } if STRUCTURED else ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if model exists\n",
    "        model_url = \"http://localhost:11434/api/tags\"\n",
    "        models = requests.get(model_url).json()\n",
    "        available_models = [model['name'] for model in models['models']]\n",
    "        \n",
    "        if MODEL_NAME not in available_models:\n",
    "            print(f\"âŒ Model '{MODEL_NAME}' not found.\")\n",
    "            return None\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 404:\n",
    "            print(\"âŒ Ollama service not running.\")\n",
    "            return None\n",
    "\n",
    "        result = response.json()\n",
    "        if STRUCTURED:\n",
    "            return ast.literal_eval(result['response'])['label']\n",
    "        return result['response']\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Cannot connect to Ollama\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_all_texts(df):\n",
    "    \"\"\"Analyze all texts in the dataframe\"\"\"\n",
    "    # Create new column for responses using model name\n",
    "    df[MODEL_NAME] = df[COLUMN_NAME].apply(query_ollama)\n",
    "    return df\n",
    "\n",
    "# Run analysis on all rows\n",
    "sample_df = analyze_all_texts(sample_df)\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d2933-8c13-4969-a986-6c8e1173a372",
   "metadata": {},
   "source": [
    "# 10. Inspect your outputs\n",
    "\n",
    "You can quickly scan your results by printing out the first N examples. Change the final integer to print more or less. Shows the passage + prompt output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11712ca-2305-4261-b62f-5e3d86c46331",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_df[[COLUMN_NAME, MODEL_NAME]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb48fd7-e9bf-456b-8007-1e27d7a0ace4",
   "metadata": {},
   "source": [
    "Print a single passage by row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896da18f-a810-46fd-b718-de55af944351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a specific row (change row_number to view different rows)\n",
    "row_number = 2  # Change this number to view different rows\n",
    "print(f\"\\nDetailed view of row {row_number}:\")\n",
    "print(f\"\\nTEXT:\\n{sample_df[COLUMN_NAME].iloc[row_number]}\")\n",
    "print(f\"\\n{MODEL_NAME} response:\\n{sample_df[MODEL_NAME].iloc[row_number]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f48fdd-7cf9-4a60-a047-838a7002f197",
   "metadata": {},
   "source": [
    "# 11. Compare your outputs to another reference column\n",
    "\n",
    "In the following cells you will compare the accuracy of your outputs to already annotated data. First you need to identify the \"reference\" column. These are the annotations. Second, you need to align your outputs with those of the reference column. Typically these will consist of a few number of codes. So the first step is finding out these codes so you can align them with your outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3828e854-8d8a-439e-9889-0fe4594f1506",
   "metadata": {},
   "source": [
    "## 12. What are the annotation categories of my data\n",
    "\n",
    "Output a table of the categories and their counts in your data. Change the reference column name accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a35d66-9ccd-4a34-bc71-7fc6a6744ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "REFERENCE_COLUMN = \"Reader.Predicted.Label\"  # Column name for reference categories\n",
    "\n",
    "########## EXECUTE ANALYSIS ###########\n",
    "reference_counts = sample_df[REFERENCE_COLUMN].value_counts()\n",
    "\n",
    "print(\"Categories and their counts:\")\n",
    "for category, count in reference_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "print(f\"\\nTotal samples: {len(sample_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ce40f-9ec4-4053-9c2d-f79ebee60a6b",
   "metadata": {},
   "source": [
    "## 13. Clean your outputs to align with reference column\n",
    "\n",
    "Here you need to input the fixed expressions you want to capture based on your prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e57389e-f5c5-401e-945c-bccb9208d5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "# Define your input-output mappings here\n",
    "CLEANING_CONFIG = {\n",
    "    'input_patterns': ['1', '0'],  # List of input patterns to match (case-insensitive)\n",
    "    'output_values': ['1', '0'],      # Corresponding output values\n",
    "    'unknown_value': 'unknown'        # Value to use when no pattern matches\n",
    "}\n",
    "\n",
    "def clean_responses(df, model_column=MODEL_NAME, config=CLEANING_CONFIG):\n",
    "    \"\"\"\n",
    "    Clean and standardize model responses based on provided configuration\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the responses to clean\n",
    "    model_column : str\n",
    "        Name of the column containing responses to clean\n",
    "    config : dict\n",
    "        Dictionary containing:\n",
    "        - input_patterns: list of strings to match (case-insensitive)\n",
    "        - output_values: corresponding output values\n",
    "        - unknown_value: value to use when no pattern matches\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with new cleaned column added\n",
    "    \"\"\"\n",
    "    def standardize_response(response):\n",
    "        # Convert response to string and lowercase for matching\n",
    "        response_str = str(response).lower().strip()\n",
    "        \n",
    "        # Try to match each input pattern\n",
    "        for pattern, value in zip(config['input_patterns'], config['output_values']):\n",
    "            if pattern.lower() in response_str:\n",
    "                return value\n",
    "                \n",
    "        # If no match found, log warning and return unknown value\n",
    "        print(f\"Warning: Unexpected response format: '{response}'\")\n",
    "        return config['unknown_value']\n",
    "            \n",
    "    # Create new cleaned column\n",
    "    cleaned_column = f\"{model_column}_cleaned\"\n",
    "    df[cleaned_column] = df[model_column].apply(standardize_response)\n",
    "    \n",
    "    # Show the counts of each category\n",
    "    cleaned_counts = df[cleaned_column].value_counts()\n",
    "    print(f\"\\nCleaned response categories:\")\n",
    "    for category, count in cleaned_counts.items():\n",
    "        print(f\"{category}: {count}\")\n",
    "    print(f\"\\nTotal samples: {len(df)}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Clean the responses using the configuration\n",
    "sample_df = clean_responses(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678ffd1b-8f56-4391-b3ce-5e1ddfcd9515",
   "metadata": {},
   "source": [
    "## 14. Make sure your reference and model labels are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d44e6-9c3e-4f42-9215-99e5131e8082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's check if the data types and values are compatible\n",
    "def check_column_compatibility(df, model_column=MODEL_NAME, reference_column=REFERENCE_COLUMN):\n",
    "    \"\"\"\n",
    "    Check if model outputs and reference labels are compatible for comparison\n",
    "    \"\"\"\n",
    "    # Get the cleaned model column name\n",
    "    model_cleaned = f\"{model_column}_cleaned\"\n",
    "    \n",
    "    # Get data types\n",
    "    ref_dtype = df[reference_column].dtype\n",
    "    model_dtype = df[model_cleaned].dtype\n",
    "    \n",
    "    # Get unique values\n",
    "    ref_values = sorted(df[reference_column].unique())\n",
    "    model_values = sorted(df[model_cleaned].unique())\n",
    "    \n",
    "    print(\"Data Type Check:\")\n",
    "    print(f\"Reference column ({reference_column}): {ref_dtype}\")\n",
    "    print(f\"Model column ({model_cleaned}): {model_dtype}\")\n",
    "    print(\"\\nUnique Values Check:\")\n",
    "    print(f\"Reference values: {ref_values}\")\n",
    "    print(f\"Model values: {model_values}\")\n",
    "    \n",
    "    # Check if types and values match\n",
    "    types_match = ref_dtype == model_dtype\n",
    "    values_match = set(ref_values) == set(model_values)\n",
    "    \n",
    "    if types_match and values_match:\n",
    "        print(\"\\nâœ… Columns are compatible! You can proceed to creating the confusion matrix.\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\nâŒ Columns need conversion. Run the conversion cell below.\")\n",
    "        return False\n",
    "\n",
    "# Run the compatibility check\n",
    "columns_compatible = check_column_compatibility(sample_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0da640-dd67-40ae-a63b-4bcac7b24389",
   "metadata": {},
   "source": [
    "## 15. Simple fix\n",
    "\n",
    "If you get an X it is most likely due to numbers / letters not aligning. Here is a simple fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ca6ac-002e-49f7-88e4-1dd5d6a34f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CONVERSION CODE ###########\n",
    "import pandas as pd\n",
    "\n",
    "def convert_columns_to_numeric(df, model_column=MODEL_NAME, reference_column=REFERENCE_COLUMN):\n",
    "    \"\"\"\n",
    "    Convert model outputs to numeric format to match reference labels\n",
    "    \"\"\"\n",
    "    # Get the cleaned model column name\n",
    "    model_cleaned = f\"{model_column}_cleaned\"\n",
    "    \n",
    "    # Convert model outputs to numeric\n",
    "    df[model_cleaned] = pd.to_numeric(df[model_cleaned])\n",
    "    \n",
    "    # Show the results\n",
    "    print(\"Updated column dtypes:\")\n",
    "    print(f\"Reference column: {df[reference_column].dtype}\")\n",
    "    print(f\"Model column: {df[model_cleaned].dtype}\")\n",
    "    print(\"\\nValue counts:\")\n",
    "    print(\"\\nReference counts:\")\n",
    "    print(df[reference_column].value_counts())\n",
    "    print(\"\\nModel counts:\")\n",
    "    print(df[model_cleaned].value_counts())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Convert columns\n",
    "sample_df = convert_columns_to_numeric(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52229b-dd49-4bd1-a0e3-4aed1348f593",
   "metadata": {},
   "source": [
    "# 16. Compare your results to the reference column\n",
    "\n",
    "This cell outputs a \"confusion matrix.\" These are great ways to observe how your model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c0c914-478b-4881-aea3-b2a4548b8ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def display_confusion_matrix(df, model_column=MODEL_NAME, reference_column=REFERENCE_COLUMN):\n",
    "    \"\"\"\n",
    "    Create and display a confusion matrix comparing model predictions to reference labels\n",
    "    using dynamic labels from the reference column\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the data\n",
    "    model_column : str\n",
    "        Name of the column containing cleaned model predictions\n",
    "    reference_column : str\n",
    "        Name of the column containing reference labels\n",
    "    \"\"\"\n",
    "    # Get the reference and cleaned model outputs\n",
    "    y_true = df[reference_column]\n",
    "    y_pred = df[f'{model_column}_cleaned']\n",
    "    \n",
    "    # Get unique values from reference column (sorted to ensure consistent order)\n",
    "    unique_labels = sorted(df[reference_column].unique())\n",
    "    \n",
    "    # Create confusion matrix with dynamic labels\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=unique_labels)\n",
    "    \n",
    "    # Create display labels based on unique values\n",
    "    display_labels = [f\"Class {label}\" for label in unique_labels]\n",
    "    \n",
    "    # Convert to pandas DataFrame for better display\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm, \n",
    "        index=[f'True {label}' for label in display_labels],\n",
    "        columns=[f'Predicted {label}' for label in display_labels]\n",
    "    )\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"Model: {model_column}\")\n",
    "    print(cm_df)\n",
    "    print(\"\\nReading the matrix:\")\n",
    "    print(f\"True Positives (Correct {display_labels[0]}): {cm[0,0]}\")\n",
    "    print(f\"False Negatives (Missed {display_labels[0]}): {cm[0,1]}\")\n",
    "    print(f\"False Positives (Wrong {display_labels[0]}): {cm[1,0]}\")\n",
    "    print(f\"True Negatives (Correct {display_labels[1]}): {cm[1,1]}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "display_confusion_matrix(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c8ff0-bcc6-4ea6-a4af-d5ac7e08ac8b",
   "metadata": {},
   "source": [
    "# Calculate Precision, Recall, and F1 Score\n",
    "\n",
    "These are measures of agreement we will use this semester to see how well a model + prompt performs. Make sure to adjust the variables at the beginning to match your goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bfd4bb-15d1-4678-85bb-e3f570e40a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "# Define your reference column and positive class\n",
    "REFERENCE_COLUMN = \"Reader.Predicted.Label\"  # Column with reference labels\n",
    "POSITIVE_CLASS = 1                          # Value that represents the positive class, i.e. the one you want to measure\n",
    "\n",
    "########## CALCULATE METRICS ###########\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(df, model_column=MODEL_NAME, reference_column=REFERENCE_COLUMN, positive_class=POSITIVE_CLASS):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1 score for model predictions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the data\n",
    "    model_column : str\n",
    "        Name of the column containing model predictions\n",
    "    reference_column : str\n",
    "        Name of the column containing reference labels\n",
    "    positive_class : int or str\n",
    "        Value that represents the positive class in your data\n",
    "    \"\"\"\n",
    "    # Get the cleaned model column name\n",
    "    model_cleaned = f\"{model_column}_cleaned\"\n",
    "    \n",
    "    # Get true and predicted labels\n",
    "    y_true = df[reference_column]\n",
    "    y_pred = df[model_cleaned]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true, y_pred, pos_label=positive_class)\n",
    "    recall = recall_score(y_true, y_pred, pos_label=positive_class)\n",
    "    f1 = f1_score(y_true, y_pred, pos_label=positive_class)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Metrics for {model_column} (positive class = {positive_class}):\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    \n",
    "    # Print interpretation\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(f\"- Precision {precision:.1%}: When the model predicts class {positive_class}, it is correct this fraction of the time\")\n",
    "    print(f\"- Recall {recall:.1%}: Of all actual class {positive_class} instances, the model found this fraction\")\n",
    "    print(f\"- F1 Score {f1:.1%}: The harmonic mean of precision and recall\")\n",
    "\n",
    "# Calculate metrics\n",
    "calculate_metrics(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fccdd3d-079c-4bc3-bf2f-778ad9552a30",
   "metadata": {},
   "source": [
    "## Inspect errors\n",
    "\n",
    "Your errors can take the form of false positives (e.g. when the model thinks a passage is a story but isn't) or false negatives (e.g. when your model thinks the passage isn't a story but is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699ed179-366a-49b5-a221-0fa5856a89af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_error_examples(df, model_column=MODEL_NAME, reference_column=REFERENCE_COLUMN, text_column='TEXT'):\n",
    "    \"\"\"\n",
    "    Display examples of false positives and false negatives with their corresponding text passages\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        DataFrame containing the data\n",
    "    model_column : str\n",
    "        Name of the column containing model predictions\n",
    "    reference_column : str\n",
    "        Name of the column containing reference labels\n",
    "    text_column : str\n",
    "        Name of the column containing text passages to display\n",
    "    \"\"\"\n",
    "    # Get the cleaned model column name\n",
    "    pred_col = f'{model_column}_cleaned'\n",
    "    \n",
    "    # Get unique values from reference column (sorted to ensure consistent order)\n",
    "    unique_labels = sorted(df[reference_column].unique())\n",
    "    \n",
    "    if len(unique_labels) != 2:\n",
    "        print(f\"Error: Expected binary classification with 2 classes, but found {len(unique_labels)} classes.\")\n",
    "        return\n",
    "        \n",
    "    positive_class = unique_labels[1]  # Usually 1 or positive class\n",
    "    negative_class = unique_labels[0]  # Usually 0 or negative class\n",
    "    \n",
    "    # Find false positives (predicted positive when actually negative)\n",
    "    false_positives = df[\n",
    "        (df[pred_col] == positive_class) & \n",
    "        (df[reference_column] == negative_class)\n",
    "    ]\n",
    "    \n",
    "    # Find false negatives (predicted negative when actually positive)\n",
    "    false_negatives = df[\n",
    "        (df[pred_col] == negative_class) & \n",
    "        (df[reference_column] == positive_class)\n",
    "    ]\n",
    "    \n",
    "    # Display one example of each if available\n",
    "    print(f\"=== FALSE POSITIVE EXAMPLE ===\")\n",
    "    print(f\"(Model incorrectly predicted {positive_class} when true label was {negative_class})\")\n",
    "    if len(false_positives) > 0:\n",
    "        fp_example = false_positives.sample(1).iloc[0]\n",
    "        print(f\"\\nPassage:\")\n",
    "        print(fp_example[text_column])\n",
    "        print(f\"\\nModel response (original):\")\n",
    "        print(fp_example[model_column])\n",
    "        print(f\"Model response (cleaned):\")\n",
    "        print(fp_example[pred_col])\n",
    "        print(f\"True label:\")\n",
    "        print(fp_example[reference_column])\n",
    "    else:\n",
    "        print(\"No false positives found!\")\n",
    "        \n",
    "    print(f\"\\n=== FALSE NEGATIVE EXAMPLE ===\")\n",
    "    print(f\"(Model incorrectly predicted {negative_class} when true label was {positive_class})\")\n",
    "    if len(false_negatives) > 0:\n",
    "        fn_example = false_negatives.sample(1).iloc[0]\n",
    "        print(f\"\\nPassage:\")\n",
    "        print(fn_example[text_column])\n",
    "        print(f\"\\nModel response (original):\")\n",
    "        print(fn_example[model_column])\n",
    "        print(f\"Model response (cleaned):\")\n",
    "        print(fn_example[pred_col])\n",
    "        print(f\"True label:\")\n",
    "        print(fn_example[reference_column])\n",
    "    else:\n",
    "        print(\"No false negatives found!\")\n",
    "\n",
    "# Show error examples\n",
    "show_error_examples(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca4e67-fe46-488b-8fb9-b8bcf14c2983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
