{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46273a7e-4863-42ba-8b34-c642a0ac2350",
   "metadata": {},
   "source": [
    "# Welcome!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c7032b-19da-420e-be87-76c671e12d3b",
   "metadata": {},
   "source": [
    "This notebook will allow you to customize prompts with different language models on your data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044b77d-8e22-4371-86b6-d98770674b28",
   "metadata": {},
   "source": [
    "# Establish Your Working Directory\n",
    "\n",
    "For our projects this semester we will upload a .csv file that has a \"text\" column. This will be our input to the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bceb944-6e7f-4e57-8f6d-88f82bf61396",
   "metadata": {},
   "source": [
    "First establish your working directory. Create a folder called \"Jupyter\" and put it in your Documents folder. Then run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b6dc77-3c12-4e5f-a3d7-40a2d19b3a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Now using your Jupyter folder!\n",
      "Current working directory: /Users/akpiper/Documents/Jupyter\n"
     ]
    }
   ],
   "source": [
    "# Import the libraries we need\n",
    "from pathlib import Path  # This helps us work with file paths\n",
    "import os                # This lets us change directories\n",
    "\n",
    "def use_jupyter_folder():\n",
    "    # Get the path to the Jupyter folder\n",
    "    jupyter_folder = Path.home() / 'Documents' / 'Jupyter'\n",
    "    \n",
    "    # Try to change to that directory\n",
    "    if jupyter_folder.exists():\n",
    "        os.chdir(jupyter_folder)\n",
    "        print(f\"âœ… Now using your Jupyter folder!\")\n",
    "        print(f\"Current working directory: {Path.cwd()}\")\n",
    "    else:\n",
    "        print(\"âŒ Couldn't find the Jupyter folder in Documents.\")\n",
    "        print(\"Please make sure you've created it first.\")\n",
    "\n",
    "# Run this to switch to the Jupyter folder\n",
    "use_jupyter_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325f954-867f-434d-8b27-c705d436d2ff",
   "metadata": {},
   "source": [
    "# Upload Your Data\n",
    "\n",
    "Next upload a .csv file of your choosing. Paste the filename where indicated at the bottom. This cell will output the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2c914d-5256-4820-a36e-272d7742766d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully loaded CharacterActions_Meta.csv\n",
      "Shape: 30 rows, 6 columns\n",
      "\n",
      "Columns in this dataset:\n",
      "- file\n",
      "- type\n",
      "- token\n",
      "- text\n",
      "- Human_Label\n",
      "- bookNLP_Label\n"
     ]
    }
   ],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "FILENAME = \"NarraDetect_Scalar.csv\"  # Your CSV filename here\n",
    "\n",
    "## Define function\n",
    "import pandas as pd\n",
    "\n",
    "def load_csv(filename):\n",
    "   \"\"\"Load CSV file and display info\"\"\"\n",
    "   try:\n",
    "       df = pd.read_csv(filename)\n",
    "       print(f\"âœ… Successfully loaded {filename}\")\n",
    "       print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "       print(\"\\nColumns in this dataset:\")\n",
    "       for col in df.columns:\n",
    "           print(f\"- {col}\")\n",
    "       return df\n",
    "   except FileNotFoundError:\n",
    "       print(f\"âŒ Could not find {filename} in {Path.cwd()}\")\n",
    "   except Exception as e:\n",
    "       print(f\"âŒ Error loading file: {str(e)}\")\n",
    "\n",
    "# Run Function\n",
    "df = load_csv(FILENAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6447f07-9dc7-44d1-8dbf-dbc385ab7ac2",
   "metadata": {},
   "source": [
    "# Inspect Your Data\n",
    "\n",
    "This cell will give you brief summary statistics on the input text column. This is the column you will use as part of your prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7651fd89-83d1-4e8c-a0c0-30d5b07d264a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š Dataset Overview:\n",
      "Total number of texts: 30\n",
      "\n",
      "ðŸ“ Text Length Statistics:\n",
      "Shortest text: 10 words\n",
      "Longest text: 89 words\n",
      "Average length: 41.6 words\n",
      "Median length: 38.0 words\n",
      "Total words in dataset: 1,248 words\n",
      "\n",
      "ðŸ“š Here are 2 example texts from your data:\n",
      "Example 1:\n",
      "Length: 19 words\n",
      "Text: The beep followed . Jack Sr . adopted a light and airy tone that belied his true mood .\n",
      "Example 2:\n",
      "Length: 48 words\n",
      "Text: I know she â€™s playing a game with me , but I do nâ€™t know the rules , and she â€™s got all the cards . Still , the hell with it â€” I just ca nâ€™t find it in me to care that I â€™m losing .\n"
     ]
    }
   ],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "TEXT_COLUMN = 'TEXT'    # Column containing text data\n",
    "NUM_EXAMPLES = 2        # Number of example texts to display\n",
    "\n",
    "########## FUNCTION DEFINITION ###########\n",
    "def text_stats(df, text_column=TEXT_COLUMN, num_examples=NUM_EXAMPLES):\n",
    "\n",
    "   \"\"\"Display text statistics and examples\"\"\"\n",
    "   # Calculate word counts\n",
    "   word_counts = df[text_column].str.split().str.len()\n",
    "   total_words = word_counts.sum()\n",
    "   \n",
    "   print(f\"ðŸ“Š Dataset Overview:\")\n",
    "   print(f\"Total number of texts: {len(df)}\")\n",
    "   \n",
    "   print(f\"\\nðŸ“ Text Length Statistics:\")\n",
    "   print(f\"Shortest text: {word_counts.min()} words\")\n",
    "   print(f\"Longest text: {word_counts.max()} words\")\n",
    "   print(f\"Average length: {word_counts.mean():.1f} words\")\n",
    "   print(f\"Median length: {word_counts.median():.1f} words\")\n",
    "   print(f\"Total words in dataset: {total_words:,} words\")\n",
    "   \n",
    "   print(f\"\\nðŸ“š Here are {num_examples} example texts from your data:\")\n",
    "   for i in range(num_examples):\n",
    "       idx = df.index[i]\n",
    "       text = df.loc[idx, text_column]\n",
    "       length = len(text.split())\n",
    "       print(f\"Example {i+1}:\")\n",
    "       print(f\"Length: {length} words\")\n",
    "       print(f\"Text: {text}\")\n",
    "\n",
    "# Calculate statistics and show examples\n",
    "text_stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a8195-5cd0-4327-b973-798635bc1c85",
   "metadata": {},
   "source": [
    "# Ensure a Model is Loaded in Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708e543-17ef-4cf0-bd62-0d36ef15d728",
   "metadata": {},
   "source": [
    "You will run this cell only once for the semester. Once the model is loaded you don't need to run it again.\n",
    "But you do need to run it every time you want to test a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "543dd310-844d-42b0-a033-0c37043e85d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3:8b\"  # Change this to your model name, e.g. \"mistral\", \"codellama\", etc.\n",
    "#model = \"deepseek-r1:7b\"\n",
    "#!ollama pull {model}\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4d8b4-655b-417d-9604-41ae9c74fd09",
   "metadata": {},
   "source": [
    "# Prompt Testing\n",
    "\n",
    "In this cell you can choose a model from Ollama and then customize a prompt. You also need to specify the text column from your .csv. The cell chooses a random passage from the .csv and outputs the answer. You can run multiple times to keep testing answers on random passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1f826ab-394d-47e1-8467-1457a56d53f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“– SAMPLE PASSAGE:\n",
      "Now she is not part of the year beginning . She has no job , she has enough money and nothing to do except this thing she lays upon herself , the commission , on which , she believes , rides her only hope of fashioning a life .\n",
      "\n",
      "ðŸ¤– MODEL RESPONSE:\n",
      "COG\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import random\n",
    "import requests\n",
    "import ast\n",
    "\n",
    "##### INPUT YOUR PARAMETERS HERE #####\n",
    "MODEL_NAME = model \n",
    "COLUMN_NAME = \"TEXT\"   # Change dataframe column name here\n",
    "PROMPT_TEMPLATE = \"Is this passage from a story? Answer yes or no {text}\" #Change your prompt here\n",
    "LABELS = [\"Yes\", \"No\"]\n",
    "\n",
    "## Define function\n",
    "\n",
    "def query_ollama(text):\n",
    "    \"\"\"Query local ollama model with text\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
    "    \n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"label\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\" : LABELS\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\n",
    "                    \"label\",\n",
    "                ]\n",
    "        } if STRUCTURED else ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if model exists\n",
    "        model_url = \"http://localhost:11434/api/tags\"\n",
    "        models = requests.get(model_url).json()\n",
    "        available_models = [model['name'] for model in models['models']]\n",
    "        \n",
    "        if MODEL_NAME not in available_models:\n",
    "            print(f\"âŒ Model '{MODEL_NAME}' not found.\")\n",
    "            print(f\"Available models: {', '.join(available_models)}\")\n",
    "            print(f\"\\nTo install {MODEL_NAME}, run this in terminal:\")\n",
    "            print(f\"ollama pull {MODEL_NAME}\")\n",
    "            return None\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 404:\n",
    "            print(\"âŒ Ollama service not running.\")\n",
    "            print(\"Start ollama by running 'ollama serve' in terminal\")\n",
    "            return None\n",
    "\n",
    "        result = response.json()\n",
    "        if STRUCTURED:\n",
    "            return ast.literal_eval(result['response'])['label']\n",
    "        return result['response']\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Cannot connect to Ollama\")\n",
    "        print(\"1. Check if Ollama is installed\") \n",
    "        print(\"2. Start Ollama by running 'ollama serve' in terminal\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_random_text(df):\n",
    "  \"\"\"Analyze a random text from dataset\"\"\"\n",
    "  random_idx = random.randint(0, len(df)-1)\n",
    "  text = df.iloc[random_idx][COLUMN_NAME]\n",
    "  print(\"\\nðŸ“– SAMPLE PASSAGE:\")\n",
    "  print(text)\n",
    "  print(\"\\nðŸ¤– MODEL RESPONSE:\")\n",
    "  return query_ollama(text)\n",
    "\n",
    "# Run\n",
    "result = analyze_random_text(df)\n",
    "if result:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423a9bf5-c088-4362-aeae-aaa88cdb2605",
   "metadata": {},
   "source": [
    "# Sample your data\n",
    "\n",
    "In this cell you will downsample your .csv file to run a mini test in class. For your final report you will run the model(s) against all rows (or a minimum sample of 100). This function allows you to determine the number of rows you sample and stores the new table. Change the variable \"n\" for number of rows to sample. \n",
    "\n",
    "** Note: every time you run this cell you will get a new random sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "960d4004-05a5-4f45-933e-586922cf29c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "SAMPLE_SIZE = 20  # Number of random texts to sample\n",
    "\n",
    "########## FUNCTION DEFINITION ###########\n",
    "def sample_texts(df, n=SAMPLE_SIZE):\n",
    "    \"\"\"\n",
    "    Sample n random rows from the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Your dataset\n",
    "    n (int): Number of samples to take\n",
    "    \"\"\"\n",
    "    global sample_df\n",
    "    sample_df = df.sample(n=n)\n",
    "    \n",
    "# Create sample with 3 rows\n",
    "sample_texts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab509a-1091-4a9d-8c54-313a63d2f137",
   "metadata": {},
   "source": [
    "# Run your prompt on your sample data\n",
    "\n",
    "In this cell you will run your prompt on the sampled data from above. The outputs will be stored as a new column named after the model you are using. In the next cell you can view those results. The cell will output \"Completed\" when complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5653fab-414c-4b53-81b6-858001c27237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "##### INPUT YOUR PARAMETERS HERE #####\n",
    "MODEL_NAME = model \n",
    "COLUMN_NAME = \"TEXT\"   # Change dataframe column name here\n",
    "PROMPT_TEMPLATE = \"Is this passage from a story? Answer only yes or no. {text}\"\n",
    "STRUCTURED = False\n",
    "LABELS = [\"Yes\", \"No\"]\n",
    "\n",
    "def query_ollama(text):\n",
    "    \"\"\"Query local ollama model with text\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
    "    \n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"label\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\" : LABELS\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\n",
    "                    \"label\",\n",
    "            ]\n",
    "        } if STRUCTURED else ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if model exists\n",
    "        model_url = \"http://localhost:11434/api/tags\"\n",
    "        models = requests.get(model_url).json()\n",
    "        available_models = [model['name'] for model in models['models']]\n",
    "        \n",
    "        if MODEL_NAME not in available_models:\n",
    "            print(f\"âŒ Model '{MODEL_NAME}' not found.\")\n",
    "            return None\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 404:\n",
    "            print(\"âŒ Ollama service not running.\")\n",
    "            return None\n",
    "\n",
    "        result = response.json()\n",
    "        if STRUCTURED:\n",
    "            return ast.literal_eval(result['response'])['label']\n",
    "        return result['response']\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"âŒ Cannot connect to Ollama\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_all_texts(df):\n",
    "    \"\"\"Analyze all texts in the dataframe\"\"\"\n",
    "    # Create new column for responses using model name\n",
    "    df[MODEL_NAME] = df[COLUMN_NAME].apply(query_ollama)\n",
    "    return df\n",
    "\n",
    "# Run analysis on all rows\n",
    "sample_df = analyze_all_texts(sample_df)\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d2933-8c13-4969-a986-6c8e1173a372",
   "metadata": {},
   "source": [
    "# Inspect your outputs\n",
    "\n",
    "You can quickly scan your results by printing out the first N examples. Change the final integer to print more or less. Shows the passage + prompt output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a11712ca-2305-4261-b62f-5e3d86c46331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  TEXT llama3:8b\n",
      "113  Situated within spitting distance of the U.S. ...       Yes\n",
      "351  \"\"Furthermore, it was guilty of laches in keep...        No\n",
      "67   The magistrates imposed a fine of Â£150 with 20...       Yes\n",
      "390  (His eyes, his glittering eyes, said Lewis.) H...       Yes\n",
      "66   â€œYour years of devoted work in the Foreign Ser...       Yes\n"
     ]
    }
   ],
   "source": [
    "print(sample_df[[COLUMN_NAME, MODEL_NAME]].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb48fd7-e9bf-456b-8007-1e27d7a0ace4",
   "metadata": {},
   "source": [
    "Print a single passage by row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "896da18f-a810-46fd-b718-de55af944351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed view of row 2:\n",
      "\n",
      "TEXT:\n",
      "The magistrates imposed a fine of Â£150 with 20 guineas (Â£21) costs. Still weak and shaken from her miscarriage, Yoko was mobbed outside the court building, one female spectator taking the opportunity to give her hair a vicious yank. The following day Unfinished Music No. 1â€”Two Virgins was released in the United Kingdom, adding an unofficial charge of indecent exposure to Johnâ€™s indictment. The brown paper cover had an allure long proven in the dirty book trade, and thousands rushed to buy the album, not to hear what extraordinary new sounds the Two Virgins had created on their first night together, but for a look at her tits and his dick.\n",
      "\n",
      "llama3:8b response:\n",
      "Yes\n"
     ]
    }
   ],
   "source": [
    "# Display a specific row (change row_number to view different rows)\n",
    "row_number = 2  # Change this number to view different rows\n",
    "print(f\"\\nDetailed view of row {row_number}:\")\n",
    "print(f\"\\nTEXT:\\n{sample_df[COLUMN_NAME].iloc[row_number]}\")\n",
    "print(f\"\\n{MODEL_NAME} response:\\n{sample_df[MODEL_NAME].iloc[row_number]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f48fdd-7cf9-4a60-a047-838a7002f197",
   "metadata": {},
   "source": [
    "# Compare your outputs to another reference column\n",
    "\n",
    "In the following cells you will compare the accuracy of your outputs to already annotated data. First you need to identify the \"reference\" column. These are the annotations. Second, you need to align your outputs with those of the reference column. Typically these will consist of a few number of codes. So the first step is finding out these codes so you can align them with your outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3828e854-8d8a-439e-9889-0fe4594f1506",
   "metadata": {},
   "source": [
    "## What are the annotation categories of my data\n",
    "\n",
    "Output a table of the categories and their counts in your data. Change the reference column name accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a35d66-9ccd-4a34-bc71-7fc6a6744ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories and their counts:\n",
      "1: 12\n",
      "0: 8\n",
      "\n",
      "Total samples: 20\n"
     ]
    }
   ],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "REFERENCE_COLUMN = \"Reader.Predicted.Label\"  # Column name for reference categories\n",
    "\n",
    "########## EXECUTE ANALYSIS ###########\n",
    "reference_counts = sample_df[REFERENCE_COLUMN].value_counts()\n",
    "\n",
    "print(\"Categories and their counts:\")\n",
    "for category, count in reference_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "print(f\"\\nTotal samples: {len(sample_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2c464-0fc3-4500-8d4b-81afd8c5cb11",
   "metadata": {},
   "source": [
    "Do the same thing for your model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8593b441-3c7f-49b8-a6a2-cf154bdbd586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response categories from llama3:8b:\n",
      "Yes: 11\n",
      "Yes.: 5\n",
      "No: 3\n",
      "No.: 1\n",
      "\n",
      "Total samples: 20\n"
     ]
    }
   ],
   "source": [
    "# Get value counts of the model responses\n",
    "model_counts = sample_df[MODEL_NAME].value_counts()\n",
    "\n",
    "print(f\"Response categories from {MODEL_NAME}:\")\n",
    "for category, count in model_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "print(f\"\\nTotal samples: {len(sample_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a389f1-4575-4799-9131-058060a90ed0",
   "metadata": {},
   "source": [
    "## Clean your model outputs\n",
    "\n",
    "In order to align your outputs with the reference column you first need to standardize your outputs. This cell outputs all of the types of response you got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76396a45-9df6-4005-8f4a-dfab038b24e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned response categories:\n",
      "Yes: 16\n",
      "No: 4\n",
      "\n",
      "Total samples: 20\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_responses(df, model_column=MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Clean and standardize model responses to 'Yes' or 'No' using case-insensitive pattern matching\n",
    "    \"\"\"\n",
    "    def standardize_response(response):\n",
    "        # Case insensitive search for 'yes' or 'no'\n",
    "        if re.search(r'yes', response, re.IGNORECASE):\n",
    "            return 'Yes'\n",
    "        elif re.search(r'no', response, re.IGNORECASE):\n",
    "            return 'No'\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected response format: '{response}'\")\n",
    "            return 'Unknown'\n",
    "            \n",
    "    # Create new cleaned column\n",
    "    cleaned_column = f\"{model_column}_cleaned\"\n",
    "    df[cleaned_column] = df[model_column].apply(standardize_response)\n",
    "    return df\n",
    "\n",
    "# Clean the responses\n",
    "sample_df = clean_responses(sample_df)\n",
    "\n",
    "# Show the new counts\n",
    "cleaned_counts = sample_df[f\"{MODEL_NAME}_cleaned\"].value_counts()\n",
    "print(f\"Cleaned response categories:\")\n",
    "for category, count in cleaned_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "print(f\"\\nTotal samples: {len(sample_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd86167-7185-4dea-9351-65d6833d2bd2",
   "metadata": {},
   "source": [
    "## Align outputs with reference column\n",
    "\n",
    "Next transform the reference column categories to match your model outputs. This is going to be a custom job every time. Paste the current code into Claude or GPT and ask for help changing the code based on your situation. Give the AI as much information as possible to help you and then paste the code back in here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c1de8b19-f7ec-4658-86c5-b6ace82efa46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference label categories:\n",
      "Yes: 12\n",
      "No: 8\n",
      "\n",
      "Total samples: 20\n"
     ]
    }
   ],
   "source": [
    "def transform_reference_labels(df):\n",
    "    \"\"\"\n",
    "    Transform reference labels from 1/0 to Yes/No\n",
    "    \"\"\"\n",
    "    def map_label(value):\n",
    "        if value == 1:\n",
    "            return 'Yes'\n",
    "        elif value == 0:\n",
    "            return 'No'\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected reference value: {value}\")\n",
    "            return 'Unknown'\n",
    "            \n",
    "    # Create new transformed column\n",
    "    reference_cleaned = \"Reader.Predicted.Label_cleaned\"\n",
    "    df[reference_cleaned] = df[\"Reader.Predicted.Label\"].apply(map_label)\n",
    "    return df\n",
    "\n",
    "# Transform reference labels\n",
    "sample_df = transform_reference_labels(sample_df)\n",
    "\n",
    "# Show the new counts\n",
    "reference_counts = sample_df[\"Reader.Predicted.Label_cleaned\"].value_counts()\n",
    "print(f\"Reference label categories:\")\n",
    "for category, count in reference_counts.items():\n",
    "    print(f\"{category}: {count}\")\n",
    "print(f\"\\nTotal samples: {len(sample_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c8ff0-bcc6-4ea6-a4af-d5ac7e08ac8b",
   "metadata": {},
   "source": [
    "# Calculate Precision, Recall, and F1 Score\n",
    "\n",
    "These are measures of agreement we will use this semester to see how well a model + prompt performs. Make sure to adjust the column names below to match your goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50bfd4bb-15d1-4678-85bb-e3f570e40a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for llama3:8b:\n",
      "Precision: 0.750\n",
      "Recall: 1.000\n",
      "F1 Score: 0.857\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def calculate_metrics(df, model_column=MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Calculate precision, recall, and F1 score for model predictions\n",
    "    \"\"\"\n",
    "    ##### CHANGE COLUMN NAMES ######\n",
    "    y_true = df['Reader.Predicted.Label_cleaned'] ### This column is the original annotated data that has been transformed.\n",
    "    y_pred = df[f'{model_column}_cleaned'] ### This is your data that has been cleaned.\n",
    "    \n",
    "    # Convert to binary format for sklearn (Yes -> 1, No -> 0)\n",
    "    y_true_binary = (y_true == 'Yes').astype(int)\n",
    "    y_pred_binary = (y_pred == 'Yes').astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_true_binary, y_pred_binary)\n",
    "    recall = recall_score(y_true_binary, y_pred_binary)\n",
    "    f1 = f1_score(y_true_binary, y_pred_binary)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Metrics for {model_column}:\")\n",
    "    print(f\"Precision: {precision:.3f}\")\n",
    "    print(f\"Recall: {recall:.3f}\")\n",
    "    print(f\"F1 Score: {f1:.3f}\")\n",
    "    \n",
    "# Calculate metrics\n",
    "calculate_metrics(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db52229b-dd49-4bd1-a0e3-4aed1348f593",
   "metadata": {},
   "source": [
    "## Output a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9c0c914-478b-4881-aea3-b2a4548b8ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "Model: llama3:8b\n",
      "          Predicted Yes  Predicted No\n",
      "True Yes             12             0\n",
      "True No               4             4\n",
      "\n",
      "Reading the matrix:\n",
      "True Positives (Correct Yes): 12\n",
      "False Negatives (Missed Yes): 0\n",
      "False Positives (Wrong Yes): 4\n",
      "True Negatives (Correct No): 4\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def display_confusion_matrix(df, model_column=MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Create and display a confusion matrix comparing model predictions to reference labels\n",
    "    \"\"\"\n",
    "    # Get the cleaned columns\n",
    "    y_true = df['Reader.Predicted.Label_cleaned']\n",
    "    y_pred = df[f'{model_column}_cleaned']\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=['Yes', 'No'])\n",
    "    \n",
    "    # Convert to pandas DataFrame for better display\n",
    "    cm_df = pd.DataFrame(\n",
    "        cm, \n",
    "        index=['True Yes', 'True No'],\n",
    "        columns=['Predicted Yes', 'Predicted No']\n",
    "    )\n",
    "    \n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(f\"Model: {model_column}\")\n",
    "    print(cm_df)\n",
    "    print(\"\\nReading the matrix:\")\n",
    "    print(f\"True Positives (Correct Yes): {cm[0,0]}\")\n",
    "    print(f\"False Negatives (Missed Yes): {cm[0,1]}\")\n",
    "    print(f\"False Positives (Wrong Yes): {cm[1,0]}\")\n",
    "    print(f\"True Negatives (Correct No): {cm[1,1]}\")\n",
    "\n",
    "# Display confusion matrix\n",
    "display_confusion_matrix(sample_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fccdd3d-079c-4bc3-bf2f-778ad9552a30",
   "metadata": {},
   "source": [
    "## Inspect errors\n",
    "\n",
    "Your errors can take the form of false positives (e.g. when the model thinks a passage is a story but isn't) or false negatives (e.g. when your model thinks the passage isn't a story but is)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "699ed179-366a-49b5-a221-0fa5856a89af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FALSE POSITIVE EXAMPLE ===\n",
      "(Model incorrectly predicted Yes when the true label was No)\n",
      "\n",
      "Passage:\n",
      "Even the faded, inconsequential and self-doubting Spirit [Geist] of the elders is more approachable than the quick-witted stupidity of junior. Even the neurotic peculiarities and malformations of the older adults represent character, that which is humanly achieved, compared with pathic health, infantilism raised to a norm. One realizes in horror that when one previously clashed with oneâ€™s parents, because they represented the world, one was secretly the mouthpiece of a still worse world against the merely bad. Unpolitical attempts to break out of the bourgeois family usually only lead to deeper entanglement in such, and sometimes it seems as if the disastrous germ-cell of society, the family, is simultaneously the nourishing germ-cell of the uncompromising will for a different one. What disintegrates, along with the family â€“ so long as the system continues â€“ is not just the most effective agency of the bourgeoisie, but also the resistance which indeed oppressed the individual, but also strengthened the latter, if not indeed producing such.\n",
      "\n",
      "Model response:\n",
      "Yes.\n",
      "\n",
      "=== FALSE NEGATIVE EXAMPLE ===\n",
      "(Model incorrectly predicted No when the true label was Yes)\n",
      "No false negatives found!\n"
     ]
    }
   ],
   "source": [
    "def show_error_examples(df, model_column=MODEL_NAME):\n",
    "    \"\"\"\n",
    "    Display examples of false positives and false negatives with their corresponding text passages\n",
    "    \"\"\"\n",
    "    # Get relevant columns\n",
    "    ref_col = 'Reader.Predicted.Label_cleaned'\n",
    "    pred_col = f'{model_column}_cleaned'\n",
    "    \n",
    "    # Find false positives (model predicted Yes when true label was No)\n",
    "    false_positives = df[\n",
    "        (df[pred_col] == 'Yes') & \n",
    "        (df[ref_col] == 'No')\n",
    "    ]\n",
    "    \n",
    "    # Find false negatives (model predicted No when true label was Yes)\n",
    "    false_negatives = df[\n",
    "        (df[pred_col] == 'No') & \n",
    "        (df[ref_col] == 'Yes')\n",
    "    ]\n",
    "    \n",
    "    # Display one example of each if available\n",
    "    print(\"=== FALSE POSITIVE EXAMPLE ===\")\n",
    "    print(\"(Model incorrectly predicted Yes when the true label was No)\")\n",
    "    if len(false_positives) > 0:\n",
    "        fp_example = false_positives.sample(1).iloc[0]\n",
    "        print(f\"\\nPassage:\")\n",
    "        print(fp_example['TEXT'])\n",
    "        print(f\"\\nModel response:\")\n",
    "        print(fp_example[model_column])\n",
    "    else:\n",
    "        print(\"No false positives found!\")\n",
    "        \n",
    "    print(\"\\n=== FALSE NEGATIVE EXAMPLE ===\")\n",
    "    print(\"(Model incorrectly predicted No when the true label was Yes)\")\n",
    "    if len(false_negatives) > 0:\n",
    "        fn_example = false_negatives.sample(1).iloc[0]\n",
    "        print(f\"\\nPassage:\")\n",
    "        print(fn_example['TEXT'])\n",
    "        print(f\"\\nModel response:\")\n",
    "        print(fn_example[model_column])\n",
    "    else:\n",
    "        print(\"No false negatives found!\")\n",
    "\n",
    "# Show error examples\n",
    "show_error_examples(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adca4e67-fe46-488b-8fb9-b8bcf14c2983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
