{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46273a7e-4863-42ba-8b34-c642a0ac2350",
   "metadata": {},
   "source": [
    "# Welcome!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "18c7032b-19da-420e-be87-76c671e12d3b",
   "metadata": {},
   "source": [
    "The goal of this week is to develop and test a narrative archetype annotation system.\n",
    "\n",
    "The dataset you will be using is called \"WikiData_Summaries_Labeled_Sample.csv\"\n",
    "\n",
    "It contains 50 summaries of famous literary works drawn from Wikipedia. It also contains annotations provided by GPT-4 for:\n",
    "\n",
    "- protagonist = main character's name\n",
    "- pro.type = hero/villain/victim\n",
    "- antagonist = the negative \"force\" of the story (corruption, violence, plague...)\n",
    "- topic = general thematic focus of the story\n",
    "- moral = one sentence story moral\n",
    "- moral.pos = three keywords that are the central morals of the story\n",
    "- moral.neg = single negative moral (x is a bad a behavior)\n",
    "- archetype = story archetype (free form based on GPT's own knowledge)\n",
    "\n",
    "You make take one of several directions for this week's report.\n",
    "\n",
    "1. Compare numerous models of different sizes on a single labeling task. How often do they agree? When they disagree, what does this tell you? Do you see semantic biases in how they respond to the task or do different models give very different answers?\n",
    "\n",
    "2. Invent your own archetype task. Here you would define a high-level dimension of narrative that you wish to classify. You can either use an open-ended approach or a pre-set taxonomy. For your report you will motivate why you chose this design and what you hoped to capture (why is it valuable?). Discuss how a given model performs on the task. You do not need to report an overall accuracy score but discuss salient examples and how they meet / fail to meet your expectations and why. Note: do not report that your model totally failed. In that case choose a different model or a different task! You can get inspiration from tv-tropes: https://tvtropes.org/pmwiki/pmwiki.php/Main/NarrativeTropes\n",
    "\n",
    "Note that for this week's report you will need to supply missing analytical steps, whether it is code to compare model outputs or code to design a prompt workflow and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56838bb1-654e-461a-93f3-1460bd95ccdb",
   "metadata": {},
   "source": [
    "# 1. Install Prerequisite Libraries\n",
    "The below code will depend on three Python \"libraries\" (software collections). Run the below cell once to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269be1d0-9531-43eb-8a48-4b4b29a01951",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044b77d-8e22-4371-86b6-d98770674b28",
   "metadata": {},
   "source": [
    "# 2. Establish Your Working Directory\n",
    "\n",
    "For our projects this semester we will upload a .csv file that has a \"text\" column. This will be our input to the language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bceb944-6e7f-4e57-8f6d-88f82bf61396",
   "metadata": {},
   "source": [
    "First establish your working directory. Create a folder called \"Jupyter\" and put it in your Documents folder. Then run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b6dc77-3c12-4e5f-a3d7-40a2d19b3a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries we need\n",
    "from pathlib import Path  # This helps us work with file paths\n",
    "import os                # This lets us change directories\n",
    "\n",
    "def use_jupyter_folder():\n",
    "    # Get the path to the Jupyter folder\n",
    "    jupyter_folder = Path.home() / 'Documents' / 'Jupyter'\n",
    "    \n",
    "    # Try to change to that directory\n",
    "    if jupyter_folder.exists():\n",
    "        os.chdir(jupyter_folder)\n",
    "        print(f\"‚úÖ Now using your Jupyter folder!\")\n",
    "        print(f\"Current working directory: {Path.cwd()}\")\n",
    "    else:\n",
    "        print(\"‚ùå Couldn't find the Jupyter folder in Documents.\")\n",
    "        print(\"Please make sure you've created it first.\")\n",
    "\n",
    "# Run this to switch to the Jupyter folder\n",
    "use_jupyter_folder()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a325f954-867f-434d-8b27-c705d436d2ff",
   "metadata": {},
   "source": [
    "# 3. Upload Your Data\n",
    "\n",
    "Next upload a .csv file. Paste the filename where indicated. This cell will output the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2c914d-5256-4820-a36e-272d7742766d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "FILENAME = \"NarraDetect_Scalar.csv\"  # Your CSV filename here\n",
    "\n",
    "## Define Function\n",
    "import pandas as pd\n",
    "\n",
    "def load_csv(filename):\n",
    "   \"\"\"Load CSV file and display info\"\"\"\n",
    "   try:\n",
    "       df = pd.read_csv(filename)\n",
    "       print(f\"‚úÖ Successfully loaded {filename}\")\n",
    "       print(f\"Shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "       print(\"\\nColumns in this dataset:\")\n",
    "       for col in df.columns:\n",
    "           print(f\"- {col}\")\n",
    "       return df\n",
    "   except FileNotFoundError:\n",
    "       print(f\"‚ùå Could not find {filename} in {Path.cwd()}\")\n",
    "   except Exception as e:\n",
    "       print(f\"‚ùå Error loading file: {str(e)}\")\n",
    "\n",
    "## Run function\n",
    "df = load_csv(FILENAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6447f07-9dc7-44d1-8dbf-dbc385ab7ac2",
   "metadata": {},
   "source": [
    "# 4. Inspect Your Data\n",
    "\n",
    "This cell will give you brief summary statistics on the input text column. This is the column you will use as part of your prompting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7651fd89-83d1-4e8c-a0c0-30d5b07d264a",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CONFIGURATION VARIABLES ###########\n",
    "TEXT_COLUMN = 'TEXT'    # Column containing text data\n",
    "NUM_EXAMPLES = 2        # Number of example texts to display\n",
    "\n",
    "########## FUNCTION DEFINITION ###########\n",
    "def text_stats(df, text_column=TEXT_COLUMN, num_examples=NUM_EXAMPLES):\n",
    "   \"\"\"Display text statistics and examples\"\"\"\n",
    "   # Calculate word counts\n",
    "   word_counts = df[text_column].str.split().str.len()\n",
    "   total_words = word_counts.sum()\n",
    "   \n",
    "   print(f\"üìä Dataset Overview:\")\n",
    "   print(f\"Total number of texts: {len(df)}\")\n",
    "   \n",
    "   print(f\"\\nüìù Text Length Statistics:\")\n",
    "   print(f\"Shortest text: {word_counts.min()} words\")\n",
    "   print(f\"Longest text: {word_counts.max()} words\")\n",
    "   print(f\"Average length: {word_counts.mean():.1f} words\")\n",
    "   print(f\"Median length: {word_counts.median():.1f} words\")\n",
    "   print(f\"Total words in dataset: {total_words:,} words\")\n",
    "   \n",
    "   print(f\"\\nüìö Here are {num_examples} example texts from your data:\")\n",
    "   for i in range(num_examples):\n",
    "       idx = df.index[i]\n",
    "       text = df.loc[idx, text_column]\n",
    "       length = len(text.split())\n",
    "       print(f\"Example {i+1}:\")\n",
    "       print(f\"Length: {length} words\")\n",
    "       print(f\"Text: {text}\")\n",
    "\n",
    "# Calculate statistics and show examples\n",
    "text_stats(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a8195-5cd0-4327-b973-798635bc1c85",
   "metadata": {},
   "source": [
    "# 5. Define your Ollama model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4708e543-17ef-4cf0-bd62-0d36ef15d728",
   "metadata": {},
   "source": [
    "You will run this cell only once for the semester. Once the model is loaded you don't need to run it again.\n",
    "But you do need to run it every time you want to test a new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "543dd310-844d-42b0-a033-0c37043e85d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3:8b\"  # Change this to your model name, e.g. \"mistral\", \"codellama\", etc.\n",
    "#!ollama pull {model}\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243f8f4-46db-4990-83df-33bf58dd645a",
   "metadata": {},
   "source": [
    "# 6. Prompt Testing\n",
    "\n",
    "In this cell you define your various parameters. These include your model, the column that has text passages, your prompt, and whether you want to use a structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2836ab-e713-4333-b94e-8a0ad1379ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### INPUT YOUR PARAMETERS HERE #####\n",
    "MODEL_NAME = model \n",
    "COLUMN_NAME = \"TEXT\"   # Change dataframe column name here\n",
    "PROMPT_TEMPLATE = \"Is this passage from a story? Answer 1 for yes or 0 for no {text}\" #Change your prompt here\n",
    "STRUCTURED = False\n",
    "LABELS = [\"1\", \"0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b4d8b4-655b-417d-9604-41ae9c74fd09",
   "metadata": {},
   "source": [
    "## 7. Test a random passage\n",
    "\n",
    "The cell chooses a random passage from the .csv and outputs the answer. You can run multiple times to keep testing answers on random passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f826ab-394d-47e1-8467-1457a56d53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import random\n",
    "import requests\n",
    "import ast\n",
    "\n",
    "def query_ollama(text):\n",
    "    \"\"\"Query local ollama model with text\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
    "    \n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"label\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\" : LABELS\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\n",
    "                    \"label\",\n",
    "                ]\n",
    "        } if STRUCTURED else ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if model exists\n",
    "        model_url = \"http://localhost:11434/api/tags\"\n",
    "        models = requests.get(model_url).json()\n",
    "        available_models = [model['name'] for model in models['models']]\n",
    "        \n",
    "        if MODEL_NAME not in available_models:\n",
    "            print(f\"‚ùå Model '{MODEL_NAME}' not found.\")\n",
    "            print(f\"Available models: {', '.join(available_models)}\")\n",
    "            print(f\"\\nTo install {MODEL_NAME}, run this in terminal:\")\n",
    "            print(f\"ollama pull {MODEL_NAME}\")\n",
    "            return None\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 404:\n",
    "            print(\"‚ùå Ollama service not running.\")\n",
    "            print(\"Start ollama by running 'ollama serve' in terminal\")\n",
    "            return None\n",
    "\n",
    "        result = response.json()\n",
    "        if STRUCTURED:\n",
    "            return ast.literal_eval(result['response'])['label']\n",
    "        return result['response']\n",
    "\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Cannot connect to Ollama\")\n",
    "        print(\"1. Check if Ollama is installed\") \n",
    "        print(\"2. Start Ollama by running 'ollama serve' in terminal\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_random_text(df):\n",
    "  \"\"\"Analyze a random text from dataset\"\"\"\n",
    "  random_idx = random.randint(0, len(df)-1)\n",
    "  text = df.iloc[random_idx][COLUMN_NAME]\n",
    "  print(\"\\nüìñ SAMPLE PASSAGE:\")\n",
    "  print(text)\n",
    "  print(\"\\nü§ñ MODEL RESPONSE:\")\n",
    "  return query_ollama(text)\n",
    "\n",
    "# Run\n",
    "result = analyze_random_text(df)\n",
    "if result:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ab509a-1091-4a9d-8c54-313a63d2f137",
   "metadata": {},
   "source": [
    "# 8. Run your prompt on all of your data\n",
    "\n",
    "In this cell you will run your prompt on the full data. The outputs will be stored as a new column named after the model you are using. In the next cell you can view those results. The cell will output \"Completed\" when complete.\n",
    "\n",
    "** Note this takes parameters from Cell 6. Prompt Testing. If you want to change them go up and rerun that cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5653fab-414c-4b53-81b6-858001c27237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def query_ollama(text):\n",
    "    \"\"\"Query local ollama model with text\"\"\"\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    prompt = PROMPT_TEMPLATE.format(text=text)\n",
    "    \n",
    "    data = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"format\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"label\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\" : LABELS\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\n",
    "                    \"label\",\n",
    "            ]\n",
    "        } if STRUCTURED else ''\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Check if model exists\n",
    "        model_url = \"http://localhost:11434/api/tags\"\n",
    "        models = requests.get(model_url).json()\n",
    "        available_models = [model['name'] for model in models['models']]\n",
    "        \n",
    "        if MODEL_NAME not in available_models:\n",
    "            print(f\"‚ùå Model '{MODEL_NAME}' not found.\")\n",
    "            return None\n",
    "\n",
    "        response = requests.post(url, json=data)\n",
    "        if response.status_code == 404:\n",
    "            print(\"‚ùå Ollama service not running.\")\n",
    "            return None\n",
    "\n",
    "        result = response.json()\n",
    "        if STRUCTURED:\n",
    "            return ast.literal_eval(result['response'])['label']\n",
    "        return result['response']\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"‚ùå Cannot connect to Ollama\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def analyze_all_texts(df):\n",
    "    \"\"\"Analyze all texts in the dataframe\"\"\"\n",
    "    # Create new column for responses using model name\n",
    "    df[MODEL_NAME] = df[COLUMN_NAME].apply(query_ollama)\n",
    "    return df\n",
    "\n",
    "# Run analysis on all rows\n",
    "sample_df = analyze_all_texts(df)\n",
    "print(\"Completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3d2933-8c13-4969-a986-6c8e1173a372",
   "metadata": {},
   "source": [
    "# 9. Inspect your outputs\n",
    "\n",
    "You can quickly scan your results by printing out the first N examples. Change the final integer to print more or less. Shows the passage + prompt output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11712ca-2305-4261-b62f-5e3d86c46331",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sample_df[[COLUMN_NAME, MODEL_NAME]].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb48fd7-e9bf-456b-8007-1e27d7a0ace4",
   "metadata": {},
   "source": [
    "Print a single passage by row number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896da18f-a810-46fd-b718-de55af944351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display a specific row (change row_number to view different rows)\n",
    "row_number = 2  # Change this number to view different rows\n",
    "print(f\"\\nDetailed view of row {row_number}:\")\n",
    "print(f\"\\nTEXT:\\n{sample_df[COLUMN_NAME].iloc[row_number]}\")\n",
    "print(f\"\\n{MODEL_NAME} response:\\n{sample_df[MODEL_NAME].iloc[row_number]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059ce40f-9ec4-4053-9c2d-f79ebee60a6b",
   "metadata": {},
   "source": [
    "## 10. Clean your outputs to standardize them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106d5808-0161-40db-8d40-114f10ebc28d",
   "metadata": {},
   "source": [
    "## 11.Plot the distribution of your labels using a bar graph\n",
    "\n",
    "Do this if you have structured, limited numbers of classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c442995e-9920-488c-aaa6-b14592e96c60",
   "metadata": {},
   "source": [
    "## 12. Compare your outputs to GPT outputs\n",
    "\n",
    "Do this if you are comparing multiple models. \n",
    "\n",
    "Options: \n",
    "\n",
    "You could measure F1, precision and recall using the GPT answers as the reference. \n",
    "\n",
    "If you are using more open-ended responses you could ask Claude/GPT to write code to analyze the semantic similarity of your outputs and compare them to GPT's outputs. Which models are closest in similarity to GPTs responses? Are there any outlier models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d593147d-931d-458b-9e86-29b1927f2150",
   "metadata": {},
   "source": [
    "## 13. Output your .csv to your Jupyter directory\n",
    "\n",
    "You can do this to inpsect your data more, review outputs or upload your data to Claude / GPT for visualization or further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d830cb24-e58c-4b83-a437-f3cebe948c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "output_file = \"sample_df_output.csv\"  # Specify the output file name\n",
    "sample_df.to_csv(output_file, index=False)  # Set index=False to exclude the DataFrame index\n",
    "\n",
    "print(f\"DataFrame saved as {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
